{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "H5JSY8HVvK9U",
        "E2L1j6tY5LfG"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWRzJ7L7Poq5"
      },
      "source": [
        "# **CSI 382 - Data Mining and Knowledge Discovery**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szfybLmlPsj1"
      },
      "source": [
        "# **Lab 10 - Association Rules**\n",
        "\n",
        "Affinity analysis is the study of attributes or characteristics that “go together.”\n",
        "Methods for affinity analysis, also known as market basket analysis, seek to uncover associations among these attributes; that is, it seeks to uncover rules for\n",
        "quantifying the relationship between two or more attributes. Association rules\n",
        "take the form “If antecedent, then consequent,” along with a measure of the support and confidence associated with the rule.\n",
        "\n",
        "For example, a particular supermarket may find that of the 1000 customers shopping on a Thursday night, 200 bought a pen, and of the 200 who bought a pen,\n",
        "50 bought paper. Thus, the association rule would be: “If buy pen, then buy\n",
        "paper,” with a support of 50/1000 = 5% and a confidence of 50/200 = 25%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQzsqmD7bLvL"
      },
      "source": [
        "# **Installing packages and importing libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGF1FmnuUv1a"
      },
      "source": [
        "!pip install squarify"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpBGKLVWQBwV"
      },
      "source": [
        "# for basic operations\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# for visualizations\n",
        "import matplotlib.pyplot as plt\n",
        "import squarify\n",
        "import seaborn as sns\n",
        "plt.style.use('fivethirtyeight')\n",
        "\n",
        "# for defining path\n",
        "import os\n",
        "\n",
        "# for market basket analysis\n",
        "from mlxtend.frequent_patterns import apriori\n",
        "from mlxtend.frequent_patterns import association_rules"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCtu167FQEU4"
      },
      "source": [
        "# **Dataset for Lab 10**\n",
        "\n",
        "**Data Set Information:**\n",
        "\n",
        "We have a dataset of a mall with 7500 transactions of different customers buying different items from the store. We have to find correlations between the different items in the store. so that we can know if a customer is buying apple, banana and mango. what is the next item, The customer would be interested in buying from the store.\n",
        "\n",
        "\n",
        "**Problem Statement**\n",
        "\n",
        "Market owners should want to know what customers will buy next by looking at the products they buy, so that market owners can adjust their product placement to increase product sales. This can be overcome by using the Apriori Algorithm to perform a Market Basket Analysis of the customer's buying behavior.\n",
        "\n",
        "The dataset can be found here in this [URL](https://drive.google.com/file/d/1DDtVOZwFQJBn0zXc69sfb2zld1PofX57/view?usp=sharing)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4yZD0OacRNd"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Fh8U31FLDD0"
      },
      "source": [
        "## **Loading the dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ifg8k1DIXMK"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/CSI 382 - Datasets/Market_Basket_Optimisation.csv', header = None)\n",
        "\n",
        "#Check number of rows and columns in the dataset\n",
        "print(\"The dataset has %d rows and %d columns.\" % df.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ooT-6JBfVCBL"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPzP4RsHGXbC"
      },
      "source": [
        "# **Dataset Visualization**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i392cBeKGUm3"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "plt.rcParams['figure.figsize'] = (15, 15)\n",
        "wordcloud = WordCloud(background_color = 'white', width = 1200,  height = 1200, max_words = 121).generate(str(df))\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis('off')\n",
        "plt.title('Most Popular Items',fontsize = 20)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFeoqhw8G4vn"
      },
      "source": [
        "The bigger words in the wordcloud depicts the most popular selling items in the supermarket."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACQWQJksG5d_"
      },
      "source": [
        "# looking at the frequency of most popular items\n",
        "\n",
        "plt.rcParams['figure.figsize'] = (18, 7)\n",
        "color = plt.cm.copper(np.linspace(0, 1, 121))\n",
        "df[0].value_counts().head(40).plot.bar(color = color)\n",
        "plt.title('frequency of most popular items', fontsize = 20)\n",
        "plt.xticks(rotation = 90 )\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNfS7Aiad9CM"
      },
      "source": [
        "Let's see a treemap implementation of the frequency of the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B14ZCrbBG_XP"
      },
      "source": [
        "\n",
        "y = df[1].value_counts().head(40).to_frame()\n",
        "\n",
        "y.index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0_zOWkPVUty"
      },
      "source": [
        "# plotting a tree map\n",
        "\n",
        "plt.rcParams['figure.figsize'] = (20, 20)\n",
        "color = plt.cm.cool(np.linspace(0, 1, 40))\n",
        "squarify.plot(sizes = y.values, label = y.index, alpha=.8, color = color)\n",
        "plt.title('Tree Map for Popular Items')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTqA21nOHBXe"
      },
      "source": [
        "Now we can check the first choices of the customers for all data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Hqup6yQHsDC"
      },
      "source": [
        "df['food'] = 'Food'\n",
        "food = df.truncate(before = -1, after = 15)\n",
        "\n",
        "import networkx as nx\n",
        "\n",
        "food = nx.from_pandas_edgelist(food, source = 'food', target = 0, edge_attr = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ksJh0ncVYKo"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "plt.rcParams['figure.figsize'] = (20, 20)\n",
        "pos = nx.spring_layout(food)\n",
        "color = plt.cm.Wistia(np.linspace(0, 15, 1))\n",
        "nx.draw_networkx_nodes(food, pos, node_size = 15000, node_color = color)\n",
        "nx.draw_networkx_edges(food, pos, width = 3, alpha = 0.6, edge_color = 'black')\n",
        "nx.draw_networkx_labels(food, pos, font_size = 20, font_family = 'sans-serif')\n",
        "plt.axis('off')\n",
        "plt.grid()\n",
        "plt.title('Top 15 First Choices', fontsize = 40)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgnoubdDeYd4"
      },
      "source": [
        "Now we can check the second choices of the customers for all data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDFYNd8GVcpG"
      },
      "source": [
        "df['secondchoice'] = 'Second Choice'\n",
        "secondchoice = df.truncate(before = -1, after = 15)\n",
        "secondchoice = nx.from_pandas_edgelist(secondchoice, source = 'food', target = 1, edge_attr = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrE8qDzWVetZ"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "plt.rcParams['figure.figsize'] = (20, 20)\n",
        "pos = nx.spring_layout(secondchoice)\n",
        "color = plt.cm.Blues(np.linspace(0, 15, 1))\n",
        "nx.draw_networkx_nodes(secondchoice, pos, node_size = 15000, node_color = color)\n",
        "nx.draw_networkx_edges(secondchoice, pos, width = 3, alpha = 0.6, edge_color = 'brown')\n",
        "nx.draw_networkx_labels(secondchoice, pos, font_size = 20, font_family = 'sans-serif')\n",
        "plt.axis('off')\n",
        "plt.grid()\n",
        "plt.title('Top 15 Second Choices', fontsize = 40)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NZD2IyTecg9"
      },
      "source": [
        "Now we can check the third choices of the customers for all data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98ls1iV6VgnA"
      },
      "source": [
        "df['thirdchoice'] = 'Third Choice'\n",
        "thirdchoice = df.truncate(before = -1, after = 10)\n",
        "thirdchoice = nx.from_pandas_edgelist(thirdchoice, source = 'food', target = 2, edge_attr = True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XZ3p6UsVieD"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "plt.rcParams['figure.figsize'] = (20, 20)\n",
        "pos = nx.spring_layout(thirdchoice)\n",
        "color = plt.cm.Reds(np.linspace(0, 15, 1))\n",
        "nx.draw_networkx_nodes(thirdchoice, pos, node_size = 15000, node_color = color)\n",
        "nx.draw_networkx_edges(thirdchoice, pos, width = 3, alpha = 0.6, edge_color = 'pink')\n",
        "nx.draw_networkx_labels(thirdchoice, pos, font_size = 20, font_family = 'sans-serif')\n",
        "plt.axis('off')\n",
        "plt.grid()\n",
        "plt.title('Top 10 Third Choices', fontsize = 40)\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSCHSX0HGBCf"
      },
      "source": [
        "# **Data Preprocessing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-xsdEkLNpO5"
      },
      "source": [
        "There are two principal methods of representing this type of market basket data:\n",
        "using either the transactional data format or the tabular data format. The transactional data format requires only two fields, an ID field and a content field,\n",
        "with each record representing a single item only.\n",
        "\n",
        "For example, the data in Table 1 could be represented using transactional data\n",
        "format as shown in Table 2. In the tabular data format, each record represents\n",
        "a separate transaction, with as many 0/1 flag fields as there are items. The data\n",
        "from Table 2 could be represented using the tabular data format, as shown in\n",
        "Figure 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNtky4HNVlBD"
      },
      "source": [
        "# making each customers shopping items an identical list\n",
        "trans = []\n",
        "for i in range(0, 7501):\n",
        "    trans.append([str(df.values[i,j]) for j in range(0, 20)])\n",
        "\n",
        "# conveting it into an numpy array\n",
        "trans = np.array(trans)\n",
        "\n",
        "# checking the shape of the array\n",
        "print(trans.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpvAA_Rw5Jwx"
      },
      "source": [
        "trans[:,0:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xg5DZC7UVnKz"
      },
      "source": [
        "import pandas as pd\n",
        "from mlxtend.preprocessing import TransactionEncoder\n",
        "\n",
        "te = TransactionEncoder()\n",
        "df = te.fit_transform(trans)\n",
        "df = pd.DataFrame(df, columns = te.columns_)\n",
        "\n",
        "# getting the shape of the data\n",
        "df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-F0baiZlchv"
      },
      "source": [
        "df.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvv1ZQIVVpBS"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# getting correlations for 121 items would be messy\n",
        "# so let's reduce the items from 121 to 40\n",
        "\n",
        "df = df.loc[:, ['mineral water', 'burgers', 'turkey', 'chocolate', 'frozen vegetables', 'spaghetti',\n",
        "                    'shrimp', 'grated cheese', 'eggs', 'cookies', 'french fries', 'herb & pepper', 'ground beef',\n",
        "                    'tomatoes', 'milk', 'escalope', 'fresh tuna', 'red wine', 'ham', 'cake', 'green tea',\n",
        "                    'whole wheat pasta', 'pancakes', 'soup', 'muffins', 'energy bar', 'olive oil', 'champagne',\n",
        "                    'avocado', 'pepper', 'butter', 'parmesan cheese', 'whole wheat rice', 'low fat yogurt',\n",
        "                    'chicken', 'vegetables mix', 'pickles', 'meatballs', 'frozen smoothie', 'yogurt cake']]\n",
        "\n",
        "# checking the shape\n",
        "df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJgP0Rq7VrIM"
      },
      "source": [
        "df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpZTJS5pVsoR"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQ2CJwlAevKC"
      },
      "source": [
        "# **Apriori Algorithm**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4_M8dhEe7WT"
      },
      "source": [
        "The algorithm was first proposed in 1994 by Rakesh Agrawal and Ramakrishnan Srikant. Apriori algorithm finds the most frequent itemsets or elements in a transaction database and identifies association rules between the items just like the above-mentioned example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Nh_p2xEfI3Y"
      },
      "source": [
        "To construct association rules between elements or items, the algorithm considers 3 important factors which are, support, confidence and lift. Each of these factors is explained as follows:\n",
        "\n",
        "**Support**:\n",
        "\n",
        "The support of item I is defined as the ratio between the number of transactions containing the item I by the total number of transactions expressed as the equation specified in the Lecture slides.\n",
        "\n",
        "**Confidence**:\n",
        "\n",
        "This is measured by the proportion of transactions with item I1, in which item I2 also appears. The confidence between two items I1 and I2, in a transaction is defined as the total number of transactions containing both items I1 and I2 divided by the total number of transactions containing I1. ( Assume I1 as X , I2 as Y )\n",
        "\n",
        "**Lift**:\n",
        "\n",
        "Lift is the ratio between the confidence and support."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtXOYbuPfdE5"
      },
      "source": [
        "**Strong Rules**\n",
        "\n",
        "Analysts may prefer rules that have either high support or high confidence, and\n",
        "usually both. Strong rules are those that meet or surpass certain minimum support and confidence criteria.\n",
        "For example, an analyst interested in finding which supermarket items are purchased together may set a minimum support level of 20% and a minimum confidence level of 70%. On the other hand, a fraud detection analyst or a terrorism\n",
        "detection analyst would need to reduce the minimum support level to 1% or less,\n",
        "since comparatively few transactions are either fraudulent or terror-related."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqRxTA3Tf2LJ"
      },
      "source": [
        "**Itemset**\n",
        "\n",
        "An itemset is a set of items contained in $I$ , and a $k-itemset$ is an itemset containing\n",
        "$k$ items. For example, \\{beans, squash\\} is a 2-itemset, and \\{broccoli, green peppers,\n",
        "corn\\} is a 3-itemset, each from the vegetable stand set $I$. The itemset frequency is\n",
        "simply the number of transactions that contain the particular itemset.\n",
        "\n",
        "A frequent\n",
        "itemset is an itemset that occurs at least a certain minimum number of times, having\n",
        "itemset frequency $\\geq \\phi$. For example, suppose that we set $\\phi = 4.$ Then itemsets that\n",
        "occur more than four times are said to be frequent. We denote the set of frequent\n",
        "$k$-itemsets as $F_{k}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r36Hz-cKgGCw"
      },
      "source": [
        "## **Finding Frequent itemsets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXYPoNSPVvYF"
      },
      "source": [
        "from mlxtend.frequent_patterns import apriori\n",
        "\n",
        "#Now, let us return the items and itemsets with at least 5% support:\n",
        "apriori(df, min_support = 0.03, use_colnames = True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eY5JH5aSVyhI"
      },
      "source": [
        "frequent_itemsets = apriori(df, min_support = 0.01, use_colnames=True)\n",
        "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
        "frequent_itemsets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgf7fB5VV0Ni"
      },
      "source": [
        "# getting th item sets with length = 2 and support more han 10%\n",
        "\n",
        "frequent_itemsets[ (frequent_itemsets['length'] >= 2) &\n",
        "                   (frequent_itemsets['support'] >= 0.01) ]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMp9cVKSgOpU"
      },
      "source": [
        "## Finding Association Rules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqWae1pxWIvj"
      },
      "source": [
        "rules_mlxtend = association_rules(frequent_itemsets, metric=\"support\", min_threshold=0.02)\n",
        "rules_mlxtend.sort_values(by=[\"support\"],ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yTMkWTxWQgr"
      },
      "source": [
        "rules_mlxtend[ (rules_mlxtend['lift'] >= 1) & (rules_mlxtend['confidence'] >= 0.3) ].sort_values(by=[\"support\"],ascending=False).head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wlboB2iYMwt"
      },
      "source": [
        "# **That's all for today!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOCb7dDxifF_"
      },
      "source": [
        "# **Tasks**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ktiGpE-0SKO"
      },
      "source": [
        "## **Dataset**\n",
        "\n",
        "This data set was produced for the purpose of analyzing the products purchased in the same basket. For more information you can follow this link - [URL](https://www.kaggle.com/ahmtcnbs/datasets-for-appiori).\n",
        "\n",
        "Download the dataset from here - [Download Link](https://drive.google.com/file/d/1bIQ0R3GC43h6TG7FlVHME7Ev8DtazM_-/view?usp=sharing)\n",
        "\n",
        "\n",
        "Now try to do the following:\n",
        "\n",
        "1. Apply appriori algorithm to explore the frequent datasets\n",
        "2. Use support,consequent support, support, confidence, lift, leverage and  conviction to measure the association rules."
      ]
    }
  ]
}