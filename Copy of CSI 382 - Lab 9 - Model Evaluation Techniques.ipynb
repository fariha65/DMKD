{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[{"file_id":"14Vfm2pJypjW5GOvIscxO6zrr9-NhFt1x","timestamp":1698660154470},{"file_id":"1-uIwOUjU8tEXSo6v4Wlk7C6C9Zqw13sn","timestamp":1637605933579},{"file_id":"1vGYD2ypcscD7sDCeZzR2wIbJ1QhzAeVM","timestamp":1637004933574},{"file_id":"1fLv1gcATGVkqVJyJ1LxxLk5jWQXQHYt-","timestamp":1636476360510},{"file_id":"18ncMGtY2tQL80wUapsVSCNHN41Qtrw1Y","timestamp":1635535081110}],"collapsed_sections":["H5JSY8HVvK9U","E2L1j6tY5LfG"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"HWRzJ7L7Poq5"},"source":["# **CSI 382 - Data Mining and Knowledge Discovery**"]},{"cell_type":"markdown","metadata":{"id":"szfybLmlPsj1"},"source":["# **Lab 9 - Model Evaluation Techniques**\n","\n","Nestled between the modeling and deployment phases comes the crucial evaluation phase, techniques for which are discussed in this Lecture. By the time we\n","arrive at the evaluation phase, the modeling phase has already generated one or\n","more candidate models. It is of critical importance that these models be evaluated for quality and effectiveness before they are deployed for use in the field.\n","\n","Deployment of data mining models usually represents a capital expenditure and\n","investment on the part of the company. If the models in question are invalid, the\n","companyâ€™s time and money are wasted."]},{"cell_type":"code","metadata":{"id":"cpBGKLVWQBwV"},"source":["# Import the packages\n","\n","import pandas as pd\n","import numpy as np\n","import itertools\n","\n","import matplotlib.pyplot as plt\n","plt.style.use('fivethirtyeight')\n","import matplotlib.gridspec as gridspec\n","import seaborn as sns\n","sns.set(style='whitegrid', palette='muted', font_scale=1.5)\n","from pylab import rcParams\n","rcParams['figure.figsize'] = 12, 8\n","import os\n","\n","# Any results you write to the current directory are saved as output."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bCtu167FQEU4"},"source":["# **Dataset for Lab 9**\n","\n","**Data Set Information:**\n","\n","This dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage.\n","\n","**Content**\n","\n","The datasets consists of several medical predictor variables and one target variable, Outcome. Predictor variables includes the number of pregnancies the patient has had, their BMI, insulin level, age, and so on.\n","\n","**Problem Statement**\n","\n","Can we build a machine learning model to accurately predict whether or not the patients in the dataset have diabetes or not?\n","\n","\n","**Acknowledgement**\n","\n","Smith, J.W., Everhart, J.E., Dickson, W.C., Knowler, W.C., & Johannes, R.S. (1988). Using the ADAP learning algorithm to forecast the onset of diabetes mellitus. In Proceedings of the Symposium on Computer Applications and Medical Care (pp. 261--265). IEEE Computer Society Press.\n","\n","\n","The dataset can be found here in this [URL](https://drive.google.com/file/d/13AMz4DPhijR7rPSTq8ih5Mg_ovh3a1TR/view?usp=sharing)"]},{"cell_type":"code","metadata":{"id":"t4yZD0OacRNd"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5Fh8U31FLDD0"},"source":["## **Loading the dataset**"]},{"cell_type":"code","metadata":{"id":"6ifg8k1DIXMK"},"source":["import pandas as pd\n","\n","df = pd.read_csv('/content/drive/MyDrive/diabetes1.csv')\n","\n","#Check number of rows and columns in the dataset\n","print(\"The dataset has %d rows and %d columns.\" % df.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FSCHSX0HGBCf"},"source":["# **Data Preprocessing**"]},{"cell_type":"markdown","metadata":{"id":"7O6--fUNNmGL"},"source":["Checking for null values"]},{"cell_type":"code","metadata":{"id":"GiYz3SpIGLc8"},"source":["df.isnull().sum()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v-xsdEkLNpO5"},"source":["Describing the statistical inference from our dataset"]},{"cell_type":"code","metadata":{"id":"1nUM54tpGNyo"},"source":["df.describe()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wmyWnh3LGQ0G"},"source":["df.info()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dPzP4RsHGXbC"},"source":["# **Exploring the dataset**\n","\n","Outcome is the target column in the dataset. Outcome '1' is with diabetes and '0' is without diabetes, we get their total counts in the dataset. Other columns in the dataset will be input to the models. Let us have a look at the count of Outcome columns."]},{"cell_type":"code","metadata":{"id":"i392cBeKGUm3"},"source":["sns.countplot(x='Outcome',data=df)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZFeoqhw8G4vn"},"source":["Basic stat summary of the feature columns against the Outcome column to see any differences between having and not having diabetes.\n","We can notice some mean differences between having diabetes and no diabetes (may not be statistically significant within certain confidence level). It will be difficult to judge though if you have many more columns."]},{"cell_type":"code","metadata":{"id":"ACQWQJksG5d_"},"source":["grouped = df.groupby('Outcome').agg({'Pregnancies':['mean', 'std', min, max],\n","                                       'Glucose':['mean', 'std', min, max],\n","                                       'BloodPressure':['mean', 'std', min, max],\n","                                       'SkinThickness':['mean', 'std', min, max],\n","                                       'Insulin':['mean', 'std', min, max],\n","                                       'BMI':['mean', 'std', min, max],\n","                                       'DiabetesPedigreeFunction':['mean', 'std', min, max],\n","                                       'Age':['mean', 'std', min, max]\n","                                      })\n","grouped.columns = [\"_\".join(x) for x in grouped.columns.ravel()]\n","grouped # or grouped.T"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1PB6vssbG-yl"},"source":["Let us have look at the distribution of the features grouping them by the Outcome column.\n","Outcome 0 is no diabetes, 1 is with diabetes."]},{"cell_type":"code","metadata":{"id":"B14ZCrbBG_XP"},"source":["plt.subplots_adjust(top=5)\n","columns=df.columns[:8]\n","plt.figure(figsize=(12,28*4))\n","gs = gridspec.GridSpec(28, 1)\n","for i, cn in enumerate(df[columns]):\n","    ax = plt.subplot(gs[i])\n","    sns.distplot(df[cn][df.Outcome == 1], bins=50)\n","    sns.distplot(df[cn][df.Outcome == 0], bins=50)\n","    ax.set_xlabel('')\n","    plt.legend(df[\"Outcome\"])\n","    ax.set_title('histogram of feature: ' + str(cn))\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cTqA21nOHBXe"},"source":["Distribution of diabetes cases are overall similar to distribution of non-diabetes distribution in each feature. No single parameter can explain the difference betweeen diabetes and non-diabetes."]},{"cell_type":"code","metadata":{"id":"0Hqup6yQHsDC"},"source":["sns.heatmap(df[df.columns[:8]].corr(),annot=True,cmap='RdYlGn')\n","fig=plt.gcf()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9T9gsMOaH3Xc"},"source":["Input parameters are not highly correlated. So we can keep all the parameters for the model. Another way to look at it is by doing pair plots."]},{"cell_type":"markdown","metadata":{"id":"OeX4ktpfYvIn"},"source":["# **Model Building**"]},{"cell_type":"code","metadata":{"id":"Hnea6L4tH_n1"},"source":["from sklearn import svm\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import KFold\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn import metrics\n","import warnings\n","warnings.filterwarnings('ignore')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gla0WEa_cTXb"},"source":["Split into train and test stratifying the 'Outcome' column.\n","This guarantees the train-test split ratio in the 'Outcome' column both in the training set and the testing set."]},{"cell_type":"markdown","metadata":{"id":"n5sXZqquOaVs"},"source":["# **Methods for evaluating the performance of a Classifier**\n"]},{"cell_type":"markdown","metadata":{"id":"CtsRzZaNOewH"},"source":["## **Holdout Method**"]},{"cell_type":"markdown","metadata":{"id":"NEsGTjPEOlH8"},"source":["In the holdout method, the original data with labeled examples is partitioned into\n","two disjoint sets, called the training and the test sets, respectively. A classifica-\n","tion model is then induced from the training set and its performance is evaluated\n","on the test set. The proportion of data reserved for training and for testing is\n","typically at the discretion of the analysts (e.g., 50-50 or two- thirds for training\n","and one-third for testing). The accuracy of the classifier can be estimated based\n","on the accuracy of the induced model on the test set."]},{"cell_type":"code","metadata":{"id":"K0F8G-plI27_"},"source":["outcome=df['Outcome']\n","data=df[df.columns[:8]]\n","\n","train,test = train_test_split(df,test_size=0.25,random_state=0,stratify=df['Outcome'])# stratify the outcome\n","\n","train_X=train[train.columns[:8]]\n","test_X=test[test.columns[:8]]\n","train_Y=train['Outcome']\n","test_Y=test['Outcome']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PXSlbXu5BcrD"},"source":["print(train_X.shape)\n","print(test_X.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u1ZFmbt8IbuT"},"source":["### **Feature Centering and Scaling**\n","\n","train_X and test_X datasets are centered to zero and normalized by the std dev. This helps in faster gradient descent.\n"]},{"cell_type":"code","metadata":{"id":"ET1ByUOjIjF5"},"source":["features = train_X.columns.values\n","\n","for feature in features:\n","    mean, std = df[feature].mean(), df[feature].std()\n","    train_X.loc[:, feature] = (train_X[feature] - mean) / std\n","    test_X.loc[:, feature] = (test_X[feature] - mean) / std"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uv-DYb3HZDIW"},"source":["### **Compare model accuracies**"]},{"cell_type":"code","metadata":{"id":"uQ9dzJG9JJ6Z"},"source":["accuracy_scores=[]\n","\n","classifiers=['Linear Svm','Radial Svm','Logistic Regression','KNN','Decision Tree', 'Random forest', 'Naive Bayes']\n","\n","models=[svm.SVC(kernel='linear'),svm.SVC(kernel='rbf'),LogisticRegression(),\n","        KNeighborsClassifier(n_neighbors=3),DecisionTreeClassifier(),\n","        RandomForestClassifier(n_estimators=100,random_state=0), GaussianNB()]\n","\n","for i in models:\n","    model = i\n","    model.fit(train_X,train_Y)\n","    prediction=model.predict(test_X)\n","    accuracy_scores.append(metrics.accuracy_score(prediction,test_Y))\n","\n","models_dataframe=pd.DataFrame(accuracy_scores,index=classifiers)\n","models_dataframe.columns=['Accuracy']\n","models_dataframe.sort_values(['Accuracy'], ascending=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WD_Vb8_rdHYh"},"source":["We see that random forest classifier has better accuracy of 80.72%. Next, Logistic Regression, Radial SVM, Decision Tree and Linear SVM has similar accuracy of more than 77%. Let us look at the feature importance in Random forest classifier."]},{"cell_type":"code","metadata":{"id":"-AIVH3qmJORG"},"source":["modelRF= RandomForestClassifier(n_estimators=100,random_state=0)\n","modelRF.fit(train_X,train_Y)\n","predictionRF=modelRF.predict(test_X)\n","pd.Series(modelRF.feature_importances_,index=train_X.columns).sort_values(ascending=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YMM8LGb_JTP3"},"source":["Top five important features are 'Glucose', 'BMI', 'Age', 'DiabetesPedigreeFunction' and 'Blood pressure'. Next, pregnancies (only in case of women) have higher chances of diabetes."]},{"cell_type":"markdown","metadata":{"id":"YPzOSlrxWPyd"},"source":["## **k-fold Cross validation**"]},{"cell_type":"markdown","metadata":{"id":"cbNzuBCNO4mA"},"source":["An alternative to random subsampling is cross-validation. In this approach, each\n","record is used the same number of times for training and exactly once for test-\n","ing. To illustrate this method, suppose we partition the data into two equal-sized\n","subsets. First, we choose one of the subsets for training and the other for testing.\n","We then swap the roles of the subsets so that the previous training set becomes\n","the test set and vice versa. This approach is called a two- fold cross-validation.\n","The total error is obtained by summing up the errors for both runs. In this exam-\n","ple, each record is used exactly once for training and once for testing."]},{"cell_type":"markdown","metadata":{"id":"2QY70kpGWa9y"},"source":["\n","\n","Most of the time, the dataset has imbalance in classes, say one or more class(es) has higher instances than the other classes. In that case, we should train the model in each and every instances of the dataset. After that we take average of all the noted accuracies over the dataset.\n","\n","For a K-fold cross validation, we first divide the dataset into K subsets.\n","Then we train on K-1 parts and test on the remainig 1 part.\n","We continue to do that by taking each part as testing and training on the remaining K-1. Accuracies and errors are then averaged to get an average accuracy of the algorithm.\n","\n","For certain subset the algorithm will underfit while for a certain other it will overfit. With K-fold cross validation we achieve a generalized model.\n","\n","Below we approach the K-fold cross validation for this dataset.\n"]},{"cell_type":"code","metadata":{"id":"f43JyINYJKVA"},"source":["from sklearn.model_selection import KFold #for K-fold cross validation\n","from sklearn.model_selection import cross_val_score #score evaluation\n","from sklearn.preprocessing import StandardScaler #Standardisation"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9-z_x8rYJi6u"},"source":["kfold = KFold(n_splits=10, random_state=42, shuffle=True) # k=10 splits the data into 10 equal parts"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ywBqKtzsJleQ"},"source":["# Starting with the original dataset and then doing centering and scaling\n","features=df[df.columns[:8]]\n","features_standard=StandardScaler().fit_transform(features)# Gaussian Standardisation\n","X=pd.DataFrame(features_standard,columns=['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness',\n","                                          'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age'])\n","X['Outcome']=df['Outcome']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qDL5ah3mJy7n"},"source":["xyz=[]\n","accuracy=[]\n","classifiers=['Linear Svm','Radial Svm','Logistic Regression','KNN','Decision Tree', 'Random forest', 'Naive Bayes']\n","models=[svm.SVC(kernel='linear'),svm.SVC(kernel='rbf'),LogisticRegression(),\n","        KNeighborsClassifier(n_neighbors=3),DecisionTreeClassifier(),\n","        RandomForestClassifier(n_estimators=100,random_state=0), GaussianNB()]\n","\n","for i in models:\n","    model = i\n","    cv_result = cross_val_score(model,X[X.columns[:8]], X['Outcome'], cv = kfold, scoring = \"accuracy\")\n","    xyz.append(cv_result.mean())\n","    accuracy.append(cv_result)\n","\n","cv_models_dataframe=pd.DataFrame(xyz, index=classifiers)\n","cv_models_dataframe.columns=['CV Mean']\n","cv_models_dataframe\n","cv_models_dataframe.sort_values(['CV Mean'], ascending=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v6nLh5m2POOk"},"source":["# **Comparing data mining methods**"]},{"cell_type":"markdown","metadata":{"id":"W5Ee60xVPVK3"},"source":["We often need to compare two different learning methods on the same problem\n","to see which is the better one to use. It seems simple: estimate the error using\n","cross-validation (or any other suitable estimation procedure), perhaps repeated\n","several times, and choose the scheme whose estimate is smaller. This is quite\n","sufficient in many practical applications: if one method has a lower estimated\n","error than another on a particular dataset, the best we can do is to use the former\n","methodâ€™s model."]},{"cell_type":"markdown","metadata":{"id":"Gw26IsCTPYh3"},"source":["However, it may be that the difference is simply caused by estimation error,\n","and in some circumstances it is important to determine whether one scheme is\n","really better than another on a particular problem. This is a standard challenge\n","for machine learning researchers. If a new learning algorithm is proposed, its\n","proponents must show that it improves on the state of the art for the problem at\n","hand and demonstrate that the observed improvement is not just a chance effect\n","in the estimation process."]},{"cell_type":"code","metadata":{"id":"AeTdFUXbJ45J"},"source":["box=pd.DataFrame(accuracy,index=[classifiers])\n","boxT = box.T"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sz3WgUyDJ73G"},"source":["ax = sns.boxplot(data=boxT, orient=\"h\", palette=\"Set2\", width=.6)\n","ax.set_yticklabels(classifiers)\n","ax.set_title('Cross validation accuracies with different classifiers')\n","ax.set_xlabel('Accuracy')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0GzTufDhKGTY"},"source":["The above plots shows that Linear SVM, Radial SVM and Logistic Regression performs better in cross validation while tree based methods, Decision Tree and Random Forrest are worse with wider distribution of their acccuracies."]},{"cell_type":"markdown","metadata":{"id":"ROTnHWAoKMBG"},"source":["# **Ensembling**"]},{"cell_type":"markdown","metadata":{"id":"sQkyPBkbKIos"},"source":["In ensemble methods, we create multiple models and then combine them that gives us better results. Enseble methods typically gives better accuracy than a single model. The models used to create such ensemble models are called base models.\n","\n","Let us do ensembling with Voting Ensemble. First we create two or more standalone models on the training dataset. A voting classifier wrap the models to get average predictions. Models with higher individual accuracies are weighted higher.\n","\n","Since KNN, Decision Tree and Random Forest models have wide range of accuracies in K-fold validation, they are not considered in ensembling the models.\n","\n","Other the models: Linear SVM, Radial SVM and Logistic Regression are combined together to get an ensemble model."]},{"cell_type":"code","metadata":{"id":"MeS-3ipKKSFD"},"source":["linear_svm=svm.SVC(kernel='linear',C=0.1,gamma=10, probability=True)\n","radial_svm=svm.SVC(kernel='rbf',C=0.1,gamma=10, probability=True)\n","lr=LogisticRegression(C=0.1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wDd6hBujKU2w"},"source":["from sklearn.ensemble import VotingClassifier #for Voting Classifier"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6vt1q3U_KZX0"},"source":["## **Ensamble with 3 classifiers combined: Linear SVM, radial SVM, Log Reg**"]},{"cell_type":"code","metadata":{"id":"Qkj5cPvfKeIH"},"source":["ensembleModel=VotingClassifier(estimators=[('Linear_svm',linear_svm), ('Radial_svm', radial_svm), ('Logistic Regression', lr)],\n","                                            voting='soft', weights=[3,1,2])\n","\n","ensembleModel.fit(train_X,train_Y)\n","predictEnsemble = ensembleModel.predict(test_X)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L5EoOuH2KhIK"},"source":["print('Accuracy of ensembled model with all the 3 classifiers is:', np.round(ensembleModel.score(test_X,test_Y), 4))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Xo8guFgosM6k"},"source":["# **ROC curve with AUC**"]},{"cell_type":"markdown","metadata":{"id":"xjzuvTm4eTqt"},"source":["Lift charts are a valuable tool, widely used in marketing. They are closely re-\n","lated to a graphical technique for evaluating data mining schemes known as ROC\n","curves, which are used in just the same situation as the preceding one, in which\n","the learner is trying to select samples of test instances that have a high propor-\n","tion of positives. The acronym stands for receiver operating characteristic, a\n","term used in signal detection to characterize the trade-off between hit rate and\n","false alarm rate over a noisy channel.\n","ROC curves depict the performance of a classifier without regard to class distri-\n","bution or error costs."]},{"cell_type":"code","metadata":{"id":"6IGfAQSQKnKM"},"source":["from sklearn import metrics\n","from sklearn.metrics import (confusion_matrix, precision_recall_curve, auc, roc_curve, recall_score,\n","                             classification_report, f1_score, average_precision_score, precision_recall_fscore_support)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y_foBzrjf_5I"},"source":["### **For the ensembling method**"]},{"cell_type":"code","metadata":{"id":"I9bA-vIcfbpx"},"source":["from sklearn.metrics import roc_curve, auc\n","false_positive_rate, true_positive_rate, thresholds = roc_curve(test_Y, predictEnsemble)\n","roc_auc = auc(false_positive_rate, true_positive_rate)\n","roc_auc"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JIgeY9GdflFs"},"source":["import matplotlib.pyplot as plt\n","plt.figure(figsize=(10,10))\n","plt.title('Receiver Operating Characteristic for ensembling method')\n","plt.plot(false_positive_rate,true_positive_rate, color='red',label = 'AUC = %0.2f' % roc_auc)\n","plt.legend(loc = 'lower right')\n","plt.plot([0, 1], [0, 1],linestyle='--')\n","plt.axis('tight')\n","plt.ylabel('True Positive Rate')\n","plt.xlabel('False Positive Rate')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ugnHNxczgJr5"},"source":["### **For all methods**"]},{"cell_type":"code","metadata":{"id":"gDUXCaMaKrOf"},"source":["# Logistic regression\n","modelLR = LogisticRegression()\n","modelLR.fit(train_X,train_Y)\n","y_pred_prob_lr = modelLR.predict_proba(test_X)[:,1]\n","fpr_lr, tpr_lr, thresholds_lr = roc_curve(test_Y, y_pred_prob_lr)\n","roc_auc_lr = auc(fpr_lr, tpr_lr)\n","precision_lr, recall_lr, th_lr = precision_recall_curve(test_Y, y_pred_prob_lr)\n","\n","# SVM with rbf\n","modelSVMrbf=svm.SVC(kernel='rbf', probability=True)\n","modelSVMrbf.fit(train_X,train_Y)\n","y_pred_prob_SVMrbf = modelSVMrbf.predict_proba(test_X)[:,1]\n","fpr_SVMrbf, tpr_SVMrbf, thresholds_SVMrbf = roc_curve(test_Y, y_pred_prob_SVMrbf)\n","roc_auc_SVMrbf = auc(fpr_SVMrbf, tpr_SVMrbf)\n","precision_SVMrbf, recall_SVMrbf, th_SVMrbf = precision_recall_curve(test_Y, y_pred_prob_SVMrbf)\n","\n","# SVM with linear\n","modelSVMlinear=svm.SVC(kernel='linear', probability=True)\n","modelSVMlinear.fit(train_X,train_Y)\n","y_pred_prob_SVMlinear = modelSVMlinear.predict_proba(test_X)[:,1]\n","fpr_SVMlinear, tpr_SVMlinear, thresholds_SVMlinear = roc_curve(test_Y, y_pred_prob_SVMlinear)\n","roc_auc_SVMlinear = auc(fpr_SVMlinear, tpr_SVMlinear)\n","precision_SVMlinear, recall_SVMlinear, th_SVMlinear = precision_recall_curve(test_Y, y_pred_prob_SVMlinear)\n","\n","# KNN\n","modelKNN = KNeighborsClassifier(n_neighbors=3)\n","modelKNN.fit(train_X,train_Y)\n","y_pred_prob_KNN = modelKNN.predict_proba(test_X)[:,1]\n","fpr_KNN, tpr_KNN, thresholds_KNN = roc_curve(test_Y, y_pred_prob_KNN)\n","roc_auc_KNN = auc(fpr_KNN, tpr_KNN)\n","precision_KNN, recall_KNN, th_KNN = precision_recall_curve(test_Y, y_pred_prob_KNN)\n","\n","\n","# Decision Tree\n","modelTree=DecisionTreeClassifier()\n","modelTree.fit(train_X,train_Y)\n","y_pred_prob_Tree = modelTree.predict_proba(test_X)[:,1]\n","fpr_Tree, tpr_Tree, thresholds_Tree = roc_curve(test_Y, y_pred_prob_Tree)\n","roc_auc_Tree = auc(fpr_Tree, tpr_Tree)\n","precision_Tree, recall_Tree, th_Tree = precision_recall_curve(test_Y, y_pred_prob_Tree)\n","\n","# Random forest\n","modelRF= RandomForestClassifier(n_estimators=100,random_state=0)\n","modelRF.fit(train_X,train_Y)\n","y_pred_prob_rf = modelRF.predict_proba(test_X)[:,1]\n","fpr_rf, tpr_rf, thresholds_rf = roc_curve(test_Y, y_pred_prob_rf)\n","roc_auc_rf = auc(fpr_rf, tpr_rf)\n","precision_rf, recall_rf, th_rf = precision_recall_curve(test_Y, y_pred_prob_rf)\n","\n","\n","# Naive Bayes\n","modelNB= GaussianNB()\n","modelNB.fit(train_X,train_Y)\n","y_pred_prob_nb = modelNB.predict_proba(test_X)[:,1]\n","fpr_nb, tpr_nb, thresholds_nb = roc_curve(test_Y, y_pred_prob_nb)\n","roc_auc_nb = auc(fpr_nb, tpr_nb)\n","precision_nb, recall_nb, th_nb = precision_recall_curve(test_Y, y_pred_prob_nb)\n","\n","# Ensemble\n","y_pred_prob_en = ensembleModel.predict_proba(test_X)[:,1]\n","fpr_en, tpr_en, thresholds_en = roc_curve(test_Y, y_pred_prob_en)\n","roc_auc_en = auc(fpr_en, tpr_en)\n","precision_en, recall_en, th_en = precision_recall_curve(test_Y, y_pred_prob_en)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mVMgVueDKwe-"},"source":["# Plot ROC curve\n","plt.plot([0, 1], [0, 1], 'k--')\n","plt.plot(fpr_lr, tpr_lr, label='Log Reg (area = %0.3f)' % roc_auc_lr)\n","plt.plot(fpr_SVMrbf, tpr_SVMrbf, label='SVM rbf (area = %0.3f)' % roc_auc_SVMrbf)\n","plt.plot(fpr_SVMlinear, tpr_SVMlinear, label='SVM linear (area = %0.3f)' % roc_auc_SVMlinear)\n","plt.plot(fpr_KNN, tpr_KNN, label='KNN (area = %0.3f)' % roc_auc_KNN)\n","plt.plot(fpr_Tree, tpr_Tree, label='Tree (area = %0.3f)' % roc_auc_Tree)\n","plt.plot(fpr_rf, tpr_rf, label='RF (area = %0.3f)' % roc_auc_rf)\n","plt.plot(fpr_nb, tpr_nb, label='NB (area = %0.3f)' % roc_auc_nb)\n","plt.plot(fpr_en, tpr_en, label='Ensamble (area = %0.3f)' % roc_auc_en)\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('ROC curves from the investigated models')\n","plt.legend(loc='best')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r5PGH3QLWjgi"},"source":["We see that Tree, KNN, RF and NB models are somewhat worse compared to the other models in AUC.\n","Ensamble model is not standing out compared to LogReg and SVM models.\n","Nevertheless, ensamble model is expected to be more robust for any future addition of training data."]},{"cell_type":"markdown","metadata":{"id":"ACm7vlZnWubo"},"source":["## **Precision-recall curve comparing the models**\n","\n","Information retrieval researchers define parameters called *recall* and *precision*:\n","\\begin{equation*}\n","    \\begin{split}\n","        {recall} &= \\frac{{number of documents retrieved that are relevant}}{{total number of documents that are relevant}}\\\\\n","        {precision} &= \\frac{{number of documents retrieved that are relevant}}{{total number of documents that are retrieved }}\n","    \\end{split}\n","\\end{equation*}\n"]},{"cell_type":"code","metadata":{"id":"0-4zQnfkJKa-"},"source":["plt.plot([1, 0], [0, 1], 'k--')\n","plt.plot(recall_lr, precision_lr, label='Log Reg')\n","plt.plot(recall_SVMrbf, precision_SVMrbf, label='SVM rbf')\n","plt.plot(recall_SVMlinear, precision_SVMlinear, label='SVM linear')\n","plt.plot(recall_KNN, precision_KNN, label='KNN')\n","plt.plot(recall_Tree, precision_Tree, label='Tree')\n","plt.plot(recall_rf, precision_rf, label='RF')\n","plt.plot(recall_nb, precision_nb, label='NB')\n","plt.plot(recall_en, precision_en, label='Ensamble')\n","plt.title('Precision vs. Recall')\n","plt.xlabel('Recall')\n","plt.ylabel('Precision')\n","plt.legend(loc='best')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tXugwZivLFo7"},"source":["With higher recall (outcome = 1 i.e. having diavetes), precision (outcome=0, i.e. no diabetes) goes down.\n","Again the ensamble model is comparable to LogReg and SVM models and other models are worse.\n"]},{"cell_type":"markdown","metadata":{"id":"Al7bNSnvPhXa"},"source":["# **Predictive Outcomes**"]},{"cell_type":"markdown","metadata":{"id":"xNTMzPyWLH_s"},"source":["## **Confusion matrix with ensemble model**\n","\n","Let us look at the confusion matrix from the ensamble classifier.\n","First define a function to plot confusion matrix"]},{"cell_type":"code","metadata":{"id":"-WaV5H91JKgz"},"source":["def plot_confusion_matrix(cm, classes,\n","                          normalize=False,\n","                          title='Confusion matrix',\n","                          cmap=plt.cm.Blues):\n","    \"\"\"\n","    This function prints and plots the confusion matrix.\n","    Normalization can be applied by setting `normalize=True`.\n","    \"\"\"\n","    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n","    plt.title(title)\n","    plt.colorbar()\n","    tick_marks = np.arange(len(classes))\n","    plt.xticks(tick_marks, classes)\n","    plt.yticks(tick_marks, classes)\n","\n","    if normalize:\n","        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","\n","    thresh = cm.max() / 2.\n","    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n","        plt.text(j, i, cm[i, j],\n","                 horizontalalignment=\"center\",\n","                 color=\"white\" if cm[i, j] > thresh else \"black\")\n","\n","    plt.tight_layout()\n","    plt.ylabel('True label')\n","    plt.xlabel('Predicted label')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8Ri5P5JzPqOE"},"source":["In the two-class case with classes yes and no, lend or not lend, mark a suspicious\n","patch as an oil slick or not, and so on, a single prediction has the four different\n","possible outcomes shown in Figure 1.\n","The true positives (TP) and true negatives (TN) are correct classifications.\n","A false positive (FP) occurs when the outcome is incorrectly predicted as yes\n","(or positive) when it is actually no (negative).\n","A false negative (FN) occurs when the outcome is incorrectly predicted as neg-\n","ative when it is actually positive."]},{"cell_type":"code","metadata":{"id":"921P52Q5LWGG"},"source":["class_names = test_Y.unique()\n","cmEnsamble = confusion_matrix(test_Y, predictEnsemble)\n","plt.grid(False)\n","plot_confusion_matrix(cmEnsamble, classes=class_names, title='Confusion matrix with ensamble model, without normalization')\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P2pURiXpPwHu"},"source":["The true positive rate is TP divided by the total number of positives, which is\n","TP + FN;\n","the false positive rate is FP divided by the total number of negatives, FP +\n","TN.\n","The overall success rate is the number of correct classifications divided by the\n","total number of classifications:\n","TP+TN\n","TP+TN+FP+FN\n","Finally, the error rate is one minus this."]},{"cell_type":"markdown","metadata":{"id":"_ESTlb3uhMY9"},"source":["## **Classification Report**"]},{"cell_type":"code","metadata":{"id":"IzlhP0gwLat2"},"source":["print(metrics.classification_report(test_Y, predictEnsemble))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oyyyMpUBLdTb"},"source":["Looking at the recall scores for 0 (no diabetes) and 1 (having diabetes), the model did well for predicting no diabetes (correct 92% of the time) while did not do that well for predicting diabetes (correct 54% of the time). So, there is room for improving prediction for diabetes."]},{"cell_type":"markdown","metadata":{"id":"uZwpSaeAP-f2"},"source":["# **Cost-sensitive learning**"]},{"cell_type":"markdown","metadata":{"id":"7HEnNspZLjU2"},"source":["## **Diabetes prediction using Neural Network with Keras**\n","\n","Keras is a high level frame work for running neural network applications. It runs tensorflow at the backend."]},{"cell_type":"markdown","metadata":{"id":"WDOWuDkGQEsa"},"source":["Suppose that you artificially increase the number of no instances by a factor of\n","10 and use the resulting dataset for training. If the learning scheme is striving to\n","minimize the number of errors, it will come up with a decision structure that is\n","biased toward avoiding errors on the no instances, because such errors are effec-\n","tively penalized 10-fold. If data with the original proportion of no instances is\n","used for testing, fewer errors will be made on these than on yes instancesâ€”that\n","is, there will be fewer false positives than false negativesâ€”because false posi-\n","tives have been weighted 10 times more heavily than false negatives.\n","\n","Varying the proportion of instances in the training set is a general technique\n","for building cost-sensitive classifiers."]},{"cell_type":"code","metadata":{"id":"o03QlydOLla-"},"source":["from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix\n","import itertools\n","\n","from keras.utils import to_categorical # convert to one-hot-encoding\n","from keras.models import Sequential\n","from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n","from keras import optimizers\n","from keras.callbacks import ReduceLROnPlateau\n","\n","# fix random seed for reproducibility\n","np.random.seed(42)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aY0ZfCk3LoeU"},"source":["train_Y = to_categorical(train_Y, num_classes = 2)\n","test_Y = to_categorical(test_Y, num_classes = 2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AoG1LZxELqjH"},"source":["# Confirm the train-test split ratio\n","print(np.shape(train_X))\n","print(np.shape(train_Y))\n","print(np.shape(test_X))\n","print(np.shape(test_Y))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PUSnlZFJLsjR"},"source":["## **1. Create the model using Keras**\n","\n","input_dim = 8 since we have 8 input variable.\n","Here we are using 5 fully connected layers defined by using the Dense class (no particular reason for 5 layers, typically more layers are better). Number of neurons in the layers are the first argument (8, 12, 12, 8, 4 & 1 respecitvely here).\n","We use the default weight initialization in Keras which is between 0 to 0.05 assuming \"uniform\" distribution.\n","First 4 layers have \"relu\" activation and output layer has \"sigmoid\" activation. Sigmoid on the output layer ensures that we have output between 0 and 1.\n","\n","We can piece it all together by adding each layer.\n"]},{"cell_type":"code","metadata":{"id":"VkcILwOnLubC"},"source":["# A regularizer that applies a L2 regularization penalty.\n","# The L2 regularization penalty is computed as: loss = l2 * reduce_sum(square(x))\n","# Try the model with and without the regularizer\n","from tensorflow.keras import regularizers\n","model = Sequential()\n","model.add(Dense(8, input_dim=8, activation='relu'))\n","model.add(Dense(12, input_dim=8, activation='relu'))\n","model.add(Dense(12, input_dim=8, activation='relu'))\n","model.add(Dense(8, activation='relu'))\n","model.add(Dense(4, activation='relu'))\n","#model.add(Dense(2,activation='sigmoid))\n","model.add(Dense(2,kernel_regularizer=regularizers.L1L2(l1=1e-5,l2=1e-4), activation='sigmoid'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x9-gId5BXs01"},"source":["model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CS6J7diSL1PL"},"source":["## **2. Compile the model**\n","\n","Now the model is ready, we can compile it (using tensorflow under the hood or backend) and train it find the best weights for prediction. loss='binary_crossentropy' since the problem is binary classification.\n","optimizer='adam' since it is efficient and default default.\n","From metrics we collect the accuracy.\n"]},{"cell_type":"code","metadata":{"id":"SpbWlJjAMnzE"},"source":["model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xJDIOxzgL786"},"source":["## **3. Fit model**\n","\n","We train the model by calling fit() on training data.\n","The number of iteration through the whole training datset is called \"epoch\". It is set to 150 (higher the better).\n","The number of instances that are evaluated before a weight update in the network is performed is the the batch size. It is set to 50 (relatively small, the dataset is also small).\n","\n","With the model.fit(), we shall also capture the accuracy each epoch.\n"]},{"cell_type":"code","metadata":{"id":"-q6rlmi8L9qo"},"source":["epoch = 150\n","batch_size = 50\n","\n","history = model.fit(train_X, train_Y, batch_size = batch_size, epochs = epoch,\n","          validation_data = (test_X, test_Y), verbose = 2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WBtGXW1-L_nj"},"source":["We see that the model accuracy hovers around 81.94% in the testing data which is not better than ensemble model (accuracy may change slightly run-to-run). We can try larger network inclear number of epochs however, very likely this is the maximum prediction or performance of the model based on the dataset. We may also end of overfitting the training dataset. Again, note that the dataset is relatively small for neural network."]},{"cell_type":"markdown","metadata":{"id":"f22YqNbVMB-q"},"source":["## **4. Evaluate model**\n","\n","We evaluate the model on test dataset and obtain the score and accuracy.\n","Score is the evaluation of the loss function for a given input.\n"]},{"cell_type":"code","metadata":{"id":"WDP3422OMEY2"},"source":["score, acc = model.evaluate(test_X, test_Y)\n","print('Test score:', score)\n","print('Test accuracy:', acc)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uJTmPtE0MG1G"},"source":["So we get 78.125% accuracy on the test dataset with this fully connected neural network (varies little bit run to run).\n","This is comparable and not significantly better than ensemble method done earlier.\n","NN acccuracy expected to improve with larger amount of training data."]},{"cell_type":"markdown","metadata":{"id":"mZ8-L9_OMKoe"},"source":["### **Training and validation curves vs. epoch**"]},{"cell_type":"code","metadata":{"id":"OiAvOAihMMaY"},"source":["# Plot the loss and accuracy curves for training and validation vs. epochs\n","\n","fig, ax = plt.subplots(2,1)\n","ax[0].plot(history.history['loss'], color='b', label=\"Training loss\")\n","ax[0].plot(history.history['val_loss'], color='r', label=\"Testing loss\",axes =ax[0])\n","legend = ax[0].legend(loc='best', shadow=True)\n","\n","ax[1].plot(history.history['accuracy'], color='b', label=\"Training accuracy\")\n","ax[1].plot(history.history['val_accuracy'], color='r',label=\"Testing accuracy\")\n","legend = ax[1].legend(loc='best', shadow=True)\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o6uBNOVfMOyI"},"source":["After about 30 epochs, testing loss starts to become higher than training loss and exactly opposite for accuracy: testing accuracy starts to get lower than training accuracy. Basically we are starting to overfit here.\n","Therefore, we get an optimal accuracy of close to 80% using this model.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"49HQ8WNvaRkQ"},"source":["### **Confusion matrix using this model**\n","\n","Let us have a look at the correct and misclasssification in the confusion matrix.\n","I am using the below function for confusion matrix."]},{"cell_type":"code","metadata":{"id":"jryEaCVuMQ8R"},"source":["# Predict the values from the validation dataset\n","Y_pred = model.predict(test_X)\n","# Convert predictions classes to one hot vectors\n","Y_pred_classes = np.argmax(Y_pred,axis = 1)\n","# Convert validation observations to one hot vectors\n","Y_true = np.argmax(test_Y,axis = 1)\n","# compute the confusion matrix\n","confusion_mtx = confusion_matrix(Y_true, Y_pred_classes)\n","# plot the confusion matrix\n","plot_confusion_matrix(confusion_mtx, classes = range(2))\n","plt.grid(False)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W-vp3MdkMUbO"},"source":["# **Conclusion**\n","\n","Close to 80% overall accuracy is obtained based on this dataset to predict diabetes in a person using ensemble model among different methods evalauted as well as NN model.\n","Non diabetes cases are predicted more accurately than diabetes cases.\n","NN model gave better prediction of diabetes cases than ensamble model (66% vs. 54%).\n","Based on Random Forest feature importance, Glucose, BMI, Age and diabetes pedigree are top causes for diabetes.\n","Based on parameter correlation, no given single parameter can predict diabetes.\n","More data, feature engineering and larger NN model is expected to improve diabetes prediction accuracy.\n","\n","** With help from several Kaggle notebooks.**"]},{"cell_type":"markdown","metadata":{"id":"8wlboB2iYMwt"},"source":["# **That's all for today!**"]},{"cell_type":"markdown","metadata":{"id":"UOCb7dDxifF_"},"source":["# **Tasks**"]},{"cell_type":"markdown","metadata":{"id":"3ktiGpE-0SKO"},"source":["## **Dataset**\n","\n","Cardiovascular diseases (CVDs) are the number 1 cause of death globally, taking an estimated 17.9 million lives each year, which accounts for 31% of all deaths worlwide.\n","\n","Heart failure is a common event caused by CVDs and this dataset contains 12 features that can be used to predict mortality by heart failure.\n","\n","Most cardiovascular diseases can be prevented by addressing behavioural risk factors such as tobacco use, unhealthy diet and obesity, physical inactivity and harmful use of alcohol using population-wide strategies.\n","\n","People with cardiovascular disease or who are at high cardiovascular risk (due to the presence of one or more risk factors such as hypertension, diabetes, hyperlipidaemia or already established disease) need early detection and management wherein a machine learning model can be of great help. For more information you can follow this link - [URL](https://archive.ics.uci.edu/ml/datasets/Heart+failure+clinical+records).\n","\n","Download the dataset from here - [Download Link](https://drive.google.com/file/d/1TZx7MkdUOcxzjF98AIpc9sC50YcTNO-2/view?usp=sharing)\n","\n","\n","Now try to do the following:\n","\n","1. Apply any number of models on the dataset\n","2. Compare the accuracies that are evident from each dataset\n","3. Find ROC, Precsion and Recall curves for each model separately\n","4. Find ROC, Precsion and Recall curves for all models combined and make a decision on the best model"]}]}