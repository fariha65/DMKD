{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[{"file_id":"1fLv1gcATGVkqVJyJ1LxxLk5jWQXQHYt-","timestamp":1695631369535},{"file_id":"18ncMGtY2tQL80wUapsVSCNHN41Qtrw1Y","timestamp":1635535081110}],"collapsed_sections":["H5JSY8HVvK9U","WiUk6FKvxXvV","E2L1j6tY5LfG"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"HWRzJ7L7Poq5"},"source":["# **CSI 382 - Data Mining and Knowledge Discovery**"]},{"cell_type":"markdown","metadata":{"id":"szfybLmlPsj1"},"source":["# **Lab 6 - Decision Trees**\n","\n","One attractive classification method involves the construction of a decision tree, a  collection  ofdecision  nodes,  connected  by branches,  extending  downward from the root node until terminating in leaf nodes.  Beginning at the root node,which by convention is placed at the top of the decision tree diagram,attributes are  tested  at  the  decision  nodes,  with  each  possible  outcome  resulting  in  a branch.  Each branch then leads either to another decision node or to a terminating leaf node."]},{"cell_type":"markdown","metadata":{"id":"bCtu167FQEU4"},"source":["# **Dataset for Lab 6**\n","\n","Car Evaluation Database was derived from a simple hierarchical decision model originally developed for the demonstration of DEX, M. Bohanec, V. Rajkovic: Expert system for decision making. Sistemica 1(1), pp. 145-157, 1990.). The model evaluates cars according to the following concept structure:\n","\n","* class: car acceptability\n","* buying: buying price\n","* maint: price of the maintenance\n","* doors: number of doors\n","* persons: capacity in terms of persons to carry\n","* lug_boot: the size of luggage boot\n","* safety: estimated safety of the car\n","\n","The Car Evaluation Database contains examples with the structural information removed, i.e., directly relates CAR to the six input attributes: buying, maint, doors, persons, lug_boot, safety.\n","\n","Because of known underlying concept structure, this database may be particularly useful for testing constructive induction and structure discovery methods.\n","\n","**Attribute Information:**\n","\n","Class Values: unacc, acc, good, vgood\n","\n","Attributes:\n","\n","* buying: vhigh, high, med, low.\n","* maint: vhigh, high, med, low.\n","* doors: 2, 3, 4, 5more.\n","* persons: 2, 4, more.\n","* lug_boot: small, med, big.\n","* safety: low, med, high.\n","\n","The dataset can be found here in this [URL](https://drive.google.com/file/d/1wzsmycx2KlW637VTBS9vKoSNYWik9Vaf/view?usp=sharing)"]},{"cell_type":"markdown","metadata":{"id":"9YNyCq-8Q36n"},"source":["**For today we need the upgraded category_encoders package. So we need to run the following code.**"]},{"cell_type":"code","metadata":{"id":"GaPM7TxTN9r_"},"source":["!pip install category_encoders"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_lOFRuZoRMhT"},"source":["## **Loading the dataset**"]},{"cell_type":"code","metadata":{"id":"qPCybQ1T0IxF"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZRtXGIHt0QRQ"},"source":["import pandas as pd\n","\n","df = pd.read_csv('/content/drive/MyDrive/car_evaluation.csv')\n","\n","#Check number of rows and columns in the dataset\n","print(\"The dataset has %d rows and %d columns.\" % df.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3H7KdBUArDrp"},"source":["# **Exploratory Data Analysis**\n","\n","Let's look into some attributes of the dataset first before preprocessing\n","\n"]},{"cell_type":"code","metadata":{"id":"0OevzX7RrtUH"},"source":["# view dimensions of dataset\n","\n","df.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nIzTbFAnrwTT"},"source":["df.head(10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mJdKlg_-fv3B"},"source":["# Check data types\n","df.info()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KyFEAkn-f1w3"},"source":["col_names = ['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'class']\n","\n","\n","for col in col_names:\n","    print(df[col].value_counts())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wzFT5uQUghpw"},"source":["df['class'].value_counts()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b0KZqJA7RUWm"},"source":["# **Dataset Preprocessing**\n","\n","We need to transform all categorical data to numerical ones. That's why we are applying some catoegory_encoder in our dataset."]},{"cell_type":"code","metadata":{"id":"Q44s4qUBglST"},"source":["# check missing values in variables\n","\n","df.isnull().sum()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tSnHH730g8Lc"},"source":["import category_encoders as ce"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c9PJkM2sg-T5"},"source":["encoder = ce.OrdinalEncoder(cols=['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety'])\n","\n","df = encoder.fit_transform(df)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JyisHShkChIo"},"source":["df.head(10)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EZq38JvOr_T_"},"source":["# **Preparing dataset to be fed into Model**\n","\n","The target/response variable in our dataset is **class**. So we are putting the class labels in our target varible $y$.\n","\n","The other varaibles/predictors are the columns **[buying, maint, doors, persons, lug_boot, safety]** and should be put in our training variable $X$."]},{"cell_type":"code","metadata":{"id":"6PsBvGTXgoBC"},"source":["X = df.drop(['class'], axis=1)\n","\n","y = df['class']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NujSBYYdgq7y"},"source":["from sklearn.model_selection import train_test_split\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 42)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yLdkjGwCguBa"},"source":["print(X_train.shape)\n","print(y_train.shape)\n","print(X_test.shape)\n","print(y_test.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1B9G6BJFhkE2"},"source":["X_train.head(10)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D3YOIngWtdpM"},"source":["# **Decision Tree - CART**\n","\n","We will now build our model of Decision Tree Classifier.\n","\n","Theclassification and regression trees(CART) method was suggested by Breimanet al.  [1] in 1984.  The decision trees produced by CART are strictly binary,containing exactly two branches for each decision node. CART recursively par-titions the records in the training data set into subsets of records with similarvalues for the target attribute.  The CART algorithm grows the tree by conduct-ing for each decision node, an exhaustive search of all available variables and allpossible splitting values, selecting the optimal split according to the followingcriteria (from Kennedy et al. [2]).\n","\n","CART (Classification and Regression Trees) is very similar to C4.5, but it differs in that it supports numerical target variables (regression) and does not compute rule sets. CART constructs binary trees using the feature and threshold that yield the largest information gain at each node.\n","\n","scikit-learn uses an optimised version of the CART algorithm; however, scikit-learn implementation does not support categorical variables for now."]},{"cell_type":"markdown","metadata":{"id":"DapBBEa81cGa"},"source":["## **Measures for selecting the Best Split**\n","\n","The measures developed for selecting the best split are often based on the degree of impurity of the child nodes. The smaller the degree of impurity, the more skewed the class distribution. For example, a node with class distribution (0,1) has zero impurity wheres a node with uniform class distribution has the highest impurity. Examples of impurity measures include:\n","   \n","* Entropy($t$) = $-\\sum_{i=0}^{c-1}{p(i|t)\\log_{2}p(i|t)}$\n","\n","* Gini($t$) = $1-\\sum_{i=0}^{c-1}{[p(i|t)]^2}$\n","\n","* Classification Error($t$) = $1-\\max_{i}[p(i|t)]$\n","\n","where, $c$ is the number of classes and $0\\log_{2}0=0$ in entropy calculations."]},{"cell_type":"code","metadata":{"id":"sy7rRVCNhnks"},"source":["# Find more about scikit-learn's implementation of decision trees here - https://scikit-learn.org/stable/modules/tree.html\n","\n","from sklearn.tree import DecisionTreeClassifier"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8Ey8tWm0hp8m"},"source":["# setting maximum depth of the decision tree to be level 7 with randomly chosen samples in the training set\n","clf_gini = DecisionTreeClassifier(max_depth=7, random_state=42)\n","\n","# fit the model\n","clf_gini.fit(X_train, y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hNc3jbr0htD6"},"source":["# Getting some predictions from the testing set\n","y_pred_gini = clf_gini.predict(X_test)\n","\n","y_pred_gini"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YuJI01gahvvC"},"source":["# Finding the testing accuracy of the model\n","from sklearn.metrics import accuracy_score\n","\n","print('Test accuracy score with criterion gini index: {0:0.4f}'. format(accuracy_score(y_test, y_pred_gini)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F1pEGN44hyyu"},"source":["# Finding the training accuracy of the model\n","y_pred_train_gini = clf_gini.predict(X_train)\n","\n","y_pred_train_gini"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TrTsqNYfh1Ta"},"source":["print('Training-set accuracy score: {0:0.4f}'. format(accuracy_score(y_train, y_pred_train_gini)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XxSJHadfh4l5"},"source":["# print the scores on training and test set\n","\n","print('Training set score: {:.4f}'.format(clf_gini.score(X_train, y_train)))\n","\n","print('Test set score: {:.4f}'.format(clf_gini.score(X_test, y_test)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"neXC2N-Qh9Cc"},"source":["# plotting the splits\n","import matplotlib.pyplot as plt\n","\n","plt.figure(figsize=(96,48))\n","\n","from sklearn import tree\n","\n","tree.plot_tree(clf_gini.fit(X_train, y_train))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1u3-oc5niFNU"},"source":["import graphviz\n","dot_data = tree.export_graphviz(clf_gini, out_file=None,\n","                              feature_names=X_train.columns,\n","                              class_names=y_train,\n","                              filled=True, rounded=True,\n","                              special_characters=True)\n","\n","graph = graphviz.Source(dot_data)\n","\n","graph"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tEhJpYRw7UD0"},"source":["# Save the figure for future reference\n","graph.render(filename='cart',directory='/content/')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x2M8I3_muj9O"},"source":["# **Evaluating the Model - CART**\n","\n","We often use a metric called confusion matrix for evaluating the accuracy of a model.\n","\n","A confusion matrix is a technique for summarizing the performance of a classification algorithm.\n","\n","Classification accuracy alone can be misleading if you have an unequal number of observations in each class or if you have more than two classes in your dataset.\n","\n","Calculating a confusion matrix can give you a better idea of what your classification model is getting right and what types of errors it is making.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"H5JSY8HVvK9U"},"source":["##**Confusion Matrix**\n","\n","A confusion matrix is basically a nxn matrix which indicates the number of instances a record has been classified as any of the n labels.\n","\n","The desired outcome for a confusion is higher values on the left to right diagonal and 0 in the right to left diagonal."]},{"cell_type":"markdown","metadata":{"id":"WiUk6FKvxXvV"},"source":["##**Calculating a Confusion Matrix**\n","\n","Below is the process for calculating a confusion Matrix.\n","\n","1. You need a test dataset or a validation dataset with expected outcome values.\n","2. Make a prediction for each row in your test dataset.\n","3. From the expected outcomes and predictions count:\n","    * The number of correct predictions for each class.\n","    * The number of incorrect predictions for each class, organized by the class that was predicted.\n","\n","These numbers are then organized into a table, or a matrix as follows:\n","\n","* Expected down the side: Each row of the matrix corresponds to a predicted class.\n","* Predicted across the top: Each column of the matrix corresponds to an actual class.\n","\n","The counts of correct and incorrect classification are then filled into the table.\n","\n","The total number of correct predictions for a class go into the expected row for that class value and the predicted column for that class value.\n","\n","In the same way, the total number of incorrect predictions for a class go into the expected row for that class value and the predicted column for that class value."]},{"cell_type":"code","metadata":{"id":"zgrxhmlFik3k"},"source":["from sklearn.metrics import confusion_matrix\n","\n","cm = confusion_matrix(y_test, y_pred_gini)\n","\n","print('Confusion matrix\\n\\n', cm)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2OOJh-Abvrnd"},"source":["from sklearn.metrics import ConfusionMatrixDisplay\n","import matplotlib.pyplot as plt\n","\n","disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=clf_gini.classes_)\n","disp.plot()\n","\n","plt.savefig('/content/drive/MyDrive/CSI 382 - Datasets/cart_confusion_matrix.png')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E2L1j6tY5LfG"},"source":["## **Support and Confidence**\n","\n","The **support** of the decision rule refers to the proportion of records in the dataset that rest in that particular terminal leaf node.The **confidence** of the rule refers to the proportion of records in the leaf nodefor which the decision rule is true."]},{"cell_type":"code","metadata":{"id":"zhspVgm8ip6-"},"source":["from sklearn.metrics import classification_report\n","\n","print(classification_report(y_test, y_pred_gini))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PxeW48ssuZbV"},"source":["# **Decision Tree - C4.5**"]},{"cell_type":"markdown","metadata":{"id":"qm2nPWqf3hjK"},"source":["The C4.5 algorithm is Quinlan’s extension of his own ID3 algorithm for generating decision trees \\cite{10.5555/152181}. Just as with CART, the C4.5 algorithm recursively visits each decision node, selecting the optimal split, until no further splits are possible. However, there are interesting differences between CART and C4.5:\n","\n","* Unlike CART, the C4.5 algorithm is not restricted to binary splits. Whereas CART always produces a binary tree, C4.5 produces a tree of more variable shape.\n","* For categorical attributes, C4.5 by default produces a separate branch for each value of the categorical attribute. This may result in more “congested” than desired, since some values may have low frequency or may naturally be associated with other values.\n","* The C4.5 method for measuring node homogeneity is quite different from the CART method and is examined in detail below.\n"]},{"cell_type":"markdown","metadata":{"id":"8UU1UQy63vpx"},"source":["**The C4.5 algorithm uses the concept of information gain or entropy reduction to select the optimal split.**\n","\n","C4.5 uses this concept of entropy as follows. Suppose that we have a candidate split $S$, which partitions the training data set $T$ into several subsets, $T_1, T_2, \\dots , T_k$.\n","The mean information requirement can then be calculated as the weighted sum of the entropies for the individual subsets, as follows:\n","\n","$H_s(T) = \\sum_{i=1}^{k}{P_{i}H_{s}(T_{i})}$\n","\n","where $P_i$ represents the proportion of records in subset $i$ . We may then define our\n","information gain to be $gain(S) = H(T) - H_S(T)$, that is, the increase in information produced by partitioning the training data $T$ according to this candidate split $S$. At\n","each decision node, C4.5 chooses the optimal split to be the split that has the greatest information gain, gain(S)."]},{"cell_type":"code","metadata":{"id":"-cEHCFVuiL6M"},"source":["# setting maximum depth of the decision tree to be level 3 with randomly chosen samples in the training set\n","clf_en = DecisionTreeClassifier(criterion='entropy', max_depth=7, random_state=42)\n","\n","# fit the model\n","clf_en.fit(X_train, y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7xedELIHiPKZ"},"source":["# Getting some predictions from the testing set\n","y_pred_en = clf_en.predict(X_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_MLUbMDniRGw"},"source":["# Getting some predictions from the training set\n","y_pred_train_en = clf_en.predict(X_train)\n","\n","y_pred_train_en"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lgLSyikWiTl5"},"source":["print('Training-set accuracy score: {0:0.4f}'. format(accuracy_score(y_train, y_pred_train_en)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mr54M6S7iVsU"},"source":["print('Training set score: {:.4f}'.format(clf_en.score(X_train, y_train)))\n","\n","print('Test set score: {:.4f}'.format(clf_en.score(X_test, y_test)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iuJSoCdoiY1Y"},"source":["plt.figure(figsize=(12,8))\n","\n","from sklearn import tree\n","\n","tree.plot_tree(clf_en.fit(X_train, y_train))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uSKaZlw7icoW"},"source":["import graphviz\n","dot_data = tree.export_graphviz(clf_en, out_file=None,\n","                              feature_names=X_train.columns,\n","                              class_names=y_train,\n","                              filled=True, rounded=True,\n","                              special_characters=True)\n","\n","graph = graphviz.Source(dot_data)\n","\n","graph"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hbbF4dD_8WqZ"},"source":["# Save the figure for future reference\n","graph.render(filename='C4.5.dot',directory='/content/drive/MyDrive/CSI 382 - Datasets/')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xsxsqQ57yCGT"},"source":["# **Evaluating the model - C4.5**"]},{"cell_type":"code","metadata":{"id":"LGdU3XC2x_s4"},"source":["from sklearn.metrics import confusion_matrix\n","\n","cm = confusion_matrix(y_test, y_pred_en)\n","\n","print('Confusion matrix\\n\\n', cm)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1pM7hOzFeuON"},"source":["from sklearn.metrics import ConfusionMatrixDisplay\n","import matplotlib.pyplot as plt\n","\n","disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=clf_en.classes_)\n","disp.plot()\n","\n","plt.savefig('/content/drive/MyDrive/CSI 382 - Datasets/cart_confusion_matrix.png')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IMfstZJ2x_s6"},"source":["from sklearn.metrics import classification_report\n","\n","print(classification_report(y_test, y_pred_en))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8wlboB2iYMwt"},"source":["# **That's all for today!**"]},{"cell_type":"markdown","metadata":{"id":"3ktiGpE-0SKO"},"source":["# **Tasks**\n","\n","**Dataset**:\n","\n","According to the World Health Organization (WHO) stroke is the 2nd leading cause of death globally, responsible for approximately 11% of total deaths.\n","This dataset is used to predict whether a patient is likely to get stroke based on the input parameters like gender, age, various diseases, and smoking status. Each row in the data provides relavant information about the patient. For learning more about the dataset, you can follow this [link](https://www.kaggle.com/fedesoriano/stroke-prediction-dataset).\n","\n","Attribute Information\n","\n","1. id: unique identifier\n","2. gender: \"Male\", \"Female\" or \"Other\"\n","3. age: age of the patient\n","4. hypertension: 0 if the patient doesn't have hypertension, 1 if the patient has hypertension\n","5. heart_disease: 0 if the patient doesn't have any heart diseases, 1 if the patient has a heart disease\n","6. ever_married: \"No\" or \"Yes\"\n","7. work_type: \"children\", \"Govt_jov\", \"Never_worked\", \"Private\" or \"Self-employed\"\n","8. Residence_type: \"Rural\" or \"Urban\"\n","9. avg_glucose_level: average glucose level in blood\n","10. bmi: body mass index\n","11. smoking_status: \"formerly smoked\", \"never smoked\", \"smokes\" or \"Unknown\"*\n","12. stroke: 1 if the patient had a stroke or 0 if not (**target**)\n","\n","*Note: \"Unknown\" in smoking_status means that the information is unavailable for this patient\n","\n","Link to the dataset - [URL](https://drive.google.com/file/d/1xtS5tHc-hkJV4SUwYsyVFmVgtGYNhN7G/view?usp=sharing)\n","\n","## **Do the following tasks**:\n","\n","1. Preprocess the dataset if required\n","2. Apply both configurations of Decision Tree algorithm\n","    * Visualize the tree for both CART and C4.5\n","3. Maximize your accuracy!!\n","4. Analyze the confusion matrix in both cases."]}]}