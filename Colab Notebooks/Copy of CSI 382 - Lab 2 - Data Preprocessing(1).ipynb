{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[{"file_id":"1ePdUtxaHy3kukotf6Bq7uISUxcKOdkPK","timestamp":1701637590765}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"M5QqfI_bZ_eA"},"source":["# **CSI 382 - Lab 2 - Data Mining and Knowledge Discovery**\n"]},{"cell_type":"markdown","metadata":{"id":"Rc_DoNbSaoNF"},"source":["\n","# **Lab 2 - Data Preprocessing**\n","\n","Data preprocessing can refer to manipulation or dropping of data before it is\n","used in order to ensure or enhance performance, and is an important step in the\n","data mining process. The phrase \"garbage in, garbage out\" is particularly appli-\n","cable to data mining and machine learning projects. Data-gathering methods are\n","often loosely controlled, resulting in out-of-range values (e.g., Income: -100),\n","impossible data combinations (e.g., Sex: Male, Pregnant: Yes), and missing val-\n","ues, etc. Analyzing data that has not been carefully screened for such problems\n","can produce misleading results. Thus, the representation and quality of data is\n","first and foremost before running any analysis."]},{"cell_type":"markdown","metadata":{"id":"iSEQCgsMabwN"},"source":["## **WHY DO WE NEED TO PREPROCESS THE DATA?**\n","\n","Much of the raw data contained in databases is unpreprocessed, incomplete, and\n","noisy. For example, the databases may contain:\n","* Fields that are obsolete or redundant\n","* Missing values\n","* Outliers\n","* Data in a form not suitable for data mining models\n","* Values not consistent with policy or common sense.\n","\n","To be useful for data mining purposes, the databases need to undergo prepro-\n","cessing, in the form of data cleaning and data transformation. Data mining often\n","deals with data that hasn’t been looked at for years, so that much of the data\n","contains field values that have expired, are no longer relevant, or are simply\n","missing."]},{"cell_type":"markdown","metadata":{"id":"5vlEmr8UbJHa"},"source":["## **HANDLING MISSING DATA**\n","\n","Missing data is a problem that continues to plague data analysis methods. Even\n","as our analysis methods gain sophistication, we continue to encounter missing\n","values in fields, especially in databases with a large number of fields. The ab-\n","sence of information is rarely beneficial. All things being equal, more data is\n","almost always better. Therefore, we should think carefully about how we handle\n","the thorny issue of missing data."]},{"cell_type":"markdown","metadata":{"id":"nXLxKQHtbV_a"},"source":["### **Loading the data from Google Drive**\n","\n","Run the following code to obtain the authorization code of allowing Google Colab to access the data file stored in your Google Drive.\n","\n","P.S.: You do not need to run this piece of code if you are running this code in a local environment or loading the dataset from a public repository."]},{"cell_type":"code","metadata":{"id":"5bLHwW3_xtAb"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4gpMXOabbznO"},"source":["## **Importing an important Library for data manipulation**\n","\n","pandas aims to be the fundamental high-level building block for doing practical, real world data analysis in Python. Additionally, it has the broader goal of becoming the most powerful and flexible open source data analysis / manipulation tool available in any language.\n","\n","Read more at: https://pandas.pydata.org/"]},{"cell_type":"code","metadata":{"id":"5rGk2bLLyYGN"},"source":["import pandas as pd"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jj-XCXPRcRIi"},"source":["## **Loading the dataset**\n","\n","The dataset for today can be found in this [URL](https://drive.google.com/file/d/1-FWxPlHngZqZ604_u5AbY7spo8CMYfDf/view?usp=sharing)\n","\n","Here we are using the function read_csv to load our dataset into a Pandas DataFrame.\n","\n","A DataFrame is the most common Structured API and simply represents a table of data with rows and columns. The list of columns and the types in those columns the schema. A simple analogy would be a spreadsheet with named columns. The fundamental difference is that while a spreadsheet sits on one computer in one specific location, a Spark DataFrame can span thousands of computers. The reason for putting the data on more than one computer should be intuitive: either the data is too large to fit on one machine or it would simply take too long to perform that computation on one machine."]},{"cell_type":"code","metadata":{"id":"zx8aReIox2Gi"},"source":["# load the dataset\n","df = pd.read_csv('/content/drive/MyDrive/Auto.csv')\n","\n","#Check number of rows and columns in the dataset\n","print(\"The dataset has %d rows and %d columns.\" % df.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tvhNfw-jcs0j"},"source":["## **Checking the data**\n","\n","Let's check out the first 10 elements of the dataset.\n","\n","The head($n$) function displays $n$ number of elements from the 'head' of the dataset, otherwise the first 10 rows of the dataset.\n","\n","Intuitively, if we used the tail(n) function, we would see elements from the 'tail' of the dataset, otherwise the last rows of the dataset."]},{"cell_type":"code","metadata":{"id":"ohuFbQSOyjDj"},"source":["df.head(10)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cLM_HeaDdoo3"},"source":["We will introduce ourselves to a new data set, the cars data set, originally compiled by Barry Becker and Ronny Kohavi of Silicon Graphics. The data set\n","consists of information about 261 automobiles manufactured in the 1970s and\n","1980s, including gas mileage, number of cylinders, cubic inches, horsepower,\n","and so on.\n","\n","*P.S.: Be advised that, the dataset that we are using today is a bit modified version of the original dataset. Do not expect the same results/values that you have seen in the theory class.*\n","\n"]},{"cell_type":"markdown","metadata":{"id":"TVTUE5UmeGbF"},"source":["## **Describing the dataset**\n","\n","Let's explore our dataset a bit by desribing the numerical fields of our dataset."]},{"cell_type":"code","metadata":{"id":"vrejuWDqd56h"},"source":["df.describe()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5j1OWLgAeUdD"},"source":["## **Finding missing values**\n","\n","In python finding missing values in a dataset is just calling an one line function. The code below displays all rows of data that has one or more columns that are empty/NaN i.e. not a number.\n","\n","Here, axis = 1 indicates that we are checking the dataset by each row, i.e. horizontally. any indicates that it may be present in any column of the dataset."]},{"cell_type":"code","metadata":{"id":"f6Mf-fsdyx7C"},"source":["missing_values = df[pd.isnull(df).any(axis=1)]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YgS75A-QfZTI"},"source":["Let's take a look at the missing data:"]},{"cell_type":"code","metadata":{"id":"2xrTM2CAy2DE"},"source":["missing_values"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P3_zHiwffeiZ"},"source":["## **Techniques in handling missing data**\n","\n","A common method of handling missing values is simply to omit from the anal-\n","ysis the records or fields with missing values. However, this may be dangerous,\n","since the pattern of missing values may in fact be systematic, and simply deleting records with missing values would lead to a biased subset of the data. Further, it seems like a waste to omit the information in all the other fields, just because one field value is missing. Therefore, data analysts have turned to methods that would replace the missing value with a value substituted according to various criteria.\n","\n","1. Replace the missing value with some constant, specified by the analyst.\n","2. Replace the missing value with the field mean (for numerical variables) or\n","the median (for categorical variables).\n","3. Replace the missing values with a value generated at random from the vari-\n","able distribution observed.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"H0t9D1ESfzqh"},"source":["### **Technique - replace with constant**\n","\n","We will simply replace all NaN values with a predefined constant. In our case, this is 0.0\n","\n","P.S.: To keep the original dataset, we are applying the change to a copy of the dataset. The name suggests the corresponding data preprocessing action that will be applied on the dataset."]},{"cell_type":"code","metadata":{"id":"p3fHdiWT-4ps"},"source":["# Replacing missing field values with user-defined constants.\n","constant_df = df.copy()\n","\n","constant_df['cylinders'] = constant_df['cylinders'].fillna(0.0, inplace=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q1P2JyIEghpZ"},"source":["Let's check again at the missing data after the first pre-processing:"]},{"cell_type":"code","metadata":{"id":"WrVzog3M_e0y"},"source":["constant_df[pd.isnull(constant_df).any(axis=1)]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JRiavb9Xgpr_"},"source":["As you see, the NaN values from the column 'cylinders' have disappeared.\n","\n","To confirm what are they actually replaced with, let's check the row 12 which had a missing cylinder."]},{"cell_type":"code","metadata":{"id":"fVpBhnEgjx0b"},"source":["constant_df.iloc[237]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"goXePl3MhFc4"},"source":["Let's continue with 'displacement' column."]},{"cell_type":"code","metadata":{"id":"87Gl2awBAefN"},"source":["constant_df['displacement'] = constant_df['displacement'].fillna(0, inplace=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f-IRji6bhlrv"},"source":["Let's check again at the missing data after the pre-processing:"]},{"cell_type":"code","metadata":{"id":"yaRM_P12AmAh"},"source":["constant_df[pd.isnull(constant_df).any(axis=1)]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Eye82jC8kRdY"},"source":["Let's continue with 'horsepower' column."]},{"cell_type":"code","metadata":{"id":"5oxCLne-ApAR"},"source":["constant_df['horsepower'] = constant_df['horsepower'].fillna(0, inplace=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8R9geScmAvI2"},"source":["constant_df[pd.isnull(constant_df).any(axis=1)]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8JWaUMYakXkf"},"source":["**Success**, no more NaN values!"]},{"cell_type":"markdown","metadata":{"id":"DJzvrwatkjIK"},"source":["### **Technique - replace with mean/median**\n","\n","We will simply replace all NaN values with the mean/median of the field that they are in. The means can be found above in the data description."]},{"cell_type":"code","metadata":{"id":"mC-v8LyOAw2h"},"source":["mean_df = df.copy()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BUfinPUrlQLO"},"source":["Let's take a look at the missing data:"]},{"cell_type":"code","metadata":{"id":"boTpUwlsBBqE"},"source":["mean_df[pd.isnull(mean_df).any(axis=1)]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"03Tkg1YQlZiJ"},"source":["Let's replace all column values with their respective mean value."]},{"cell_type":"code","metadata":{"id":"SjL5PohOA9KV"},"source":["mean_df = mean_df.fillna(mean_df.mean())"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mean_df.iloc[353]"],"metadata":{"id":"8Ae7tke1rrG7"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZFppKuMCBSER"},"source":["mean_df[pd.isnull(mean_df).any(axis=1)]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wNXFSydVlijx"},"source":["**Success**, no more NaN values!"]},{"cell_type":"markdown","metadata":{"id":"RZPDP5zUlnyu"},"source":["To confirm what are they actually replaced with, let's check the row 32 which had a missing horsepower."]},{"cell_type":"code","metadata":{"id":"WKiI8n1HBddi"},"source":["mean_df.iloc[32]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5-b6svR4l3QP"},"source":["### **Technique - replace with random draws from the distribution of the variable**\n","\n","Here we will use uniform randomness to generate numbers from the mean and standard deviation of the fields"]},{"cell_type":"code","metadata":{"id":"Fkk_TWwPB4tC"},"source":["import numpy as np\n","\n","def fillNaN_with_unifrand(df):\n","    a = df.values\n","    # Checking how many isNaNs are there\n","    m = np.isnan(a) # mask of NaNs\n","    # Generating varables for passing to normal distribution\n","    mu, sigma = df.mean(), df.std()\n","    a[m] = np.random.normal(mu, sigma, size=m.sum())\n","    return df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MWKtrZ95B7kH"},"source":["random_df = df.copy()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hoC4cFLomhCk"},"source":["Let's take a look at the missing data:"]},{"cell_type":"code","metadata":{"id":"ntI3necMB_bH"},"source":["random_df[pd.isnull(random_df).any(axis=1)]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ueI8y11of1b1"},"source":["Let's replace 'cylinder' column values with their random values from their mean and standard deviation."]},{"cell_type":"code","metadata":{"id":"NCyWA6qGCLUU"},"source":["random_df['cylinders'] = fillNaN_with_unifrand(random_df['cylinders'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cIJ3KWmJFICg"},"source":["random_df[pd.isnull(random_df).any(axis=1)]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ITJ092SJgG6x"},"source":["To confirm what are they actually replaced with, let's check the row 12 which had a missing cylinder."]},{"cell_type":"code","metadata":{"id":"66jdzueuFOhn"},"source":["random_df.iloc[12]"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["random_df.iloc[237]"],"metadata":{"id":"XxphjuAWuHr2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H9qMujgugTrn"},"source":["Let's replace 'displacement' column values with their random values from their mean and standard deviation."]},{"cell_type":"code","metadata":{"id":"qN6rmhKaHmqX"},"source":["random_df['displacement'] = fillNaN_with_unifrand(random_df['displacement'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"grac-jEcHw1O"},"source":["random_df[pd.isnull(random_df).any(axis=1)]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Bx9bCEYGgcXd"},"source":["Let's replace 'horsepower' column values with their random values from their mean and standard deviation."]},{"cell_type":"code","metadata":{"id":"n1aPTABEH0Tc"},"source":["random_df['horsepower'] = fillNaN_with_unifrand(random_df['horsepower'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4mJuioPIH3Xj"},"source":["random_df[pd.isnull(random_df).any(axis=1)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["random_df.iloc[336]"],"metadata":{"id":"M6J_KnzWucyt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zwYXUHnzgqz6"},"source":["**Success**, no more NaN values!"]},{"cell_type":"markdown","metadata":{"id":"dEeanGlUgtjP"},"source":["## **Finding outliers**"]},{"cell_type":"markdown","metadata":{"id":"NDSYCSnJnhjR"},"source":["### **Histograms**\n","\n","One graphical method for identifying outliers for numeric variables is to exam-\n","ine a histogram of the variable. Figure below shows a histogram generated of the\n","vehicle weights from the cars data set. There appears to be one lonely vehicle in the extreme left tail of the distribution, with a vehicle weight in the hundreds of pounds rather than in the thousands."]},{"cell_type":"code","metadata":{"id":"PNAr7w1EKEwr"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from matplotlib.pyplot import figure\n","from matplotlib.ticker import PercentFormatter\n","\n","figure(figsize=(16, 6), dpi=80)\n","\n","# the histogram of the data\n","plt.hist(df['weight'],25, facecolor='b')\n","\n","plt.xlabel('weight')\n","plt.ylabel('Count')\n","plt.title('Histogram of weight')\n","plt.grid(True)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nfCrb8CinzsC"},"source":["### **Scatterplots**\n","\n","Sometimes two-dimensional scatter plots can help to reveal outliers in more than\n","one variable. The scatter plot of mpg against weightlbs shown in Figure 6 seems\n","to have netted two outliers."]},{"cell_type":"code","metadata":{"id":"QyNeEtMUOq36"},"source":["figure(figsize=(16, 6), dpi=80)\n","\n","plt.scatter(df['weight'], df['mpg'])\n","\n","plt.xlabel('weight')\n","plt.ylabel('mpg')\n","\n","plt.title('Scatter plot of mpg against weight')\n","\n","plt.grid(True)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UGL0ID9jn9SJ"},"source":["## **Data Transformation**\n","\n","Variables tend to have ranges that vary greatly from each other. For example, if\n","we are interested in major league baseball, players’ batting averages will range\n","from zero to less than 0.400, while the number of home runs hit in a season\n","will range from zero to around 70. For some data mining algorithms, such\n","differences in the ranges will lead to a tendency for the variable with greater\n","range to have undue influence on the results.\n","\n","Therefore, data miners should normalize their numerical variables, to standardize the scale of effect each variable has on the results. There are several techniques for normalization, and we shall examine two of the more prevalent methods. Let X refer to our original field value and X* refer to the normalized field value."]},{"cell_type":"markdown","metadata":{"id":"hC3QaADLoIzx"},"source":["### **Min-Max Normalization**\n","\n","Min–max normalization works by seeing how much greater the ﬁeld value is than the minimum value min(X) and scaling this difference by the range. That is,\n","\n","\\begin{equation}\n","X^{*} = \\frac{X-min(x)}{range(X)} = \\frac{X-min(X)}{max(X)-min(X)}\n","\\end{equation}\n","\n","For example, consider the acceleration variable from the cars data set, which\n","measures how long (in seconds) each automobile takes to reach 60 miles per hour.\n","Let’s ﬁnd the min–max normalization for three automobiles having times-to-60 of 8,\n","15.548, seconds, and 25 seconds, respectively."]},{"cell_type":"markdown","metadata":{"id":"a_OSKhVto3SM"},"source":["Let's check the current condition of the field before any normalization:"]},{"cell_type":"code","metadata":{"id":"RTWrz2VnPIOH"},"source":["df['acceleration'].describe()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZFS40sblo949"},"source":["Let's apply the equation that we have formulated for finding the normalized value:"]},{"cell_type":"code","metadata":{"id":"WjHUvrmVTv4T"},"source":["normalized_df = df.copy()\n","\n","normalized_df['acceleration']=(df['acceleration']-df['acceleration'].min())/(df['acceleration'].max()-df['acceleration'].min())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jLDDODk2pK4O"},"source":["Let's check the normalized value for the column acceleration:\n"]},{"cell_type":"code","metadata":{"id":"L7xMYrCHVsDl"},"source":["normalized_df['acceleration'].describe()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cQHk59cSpVXB"},"source":["### **Z-Score Standardization**\n","\n","Z-score standardization, which is very widespread in the world of statistical analysis, works by taking the difference between the ﬁeld value and the ﬁeld mean value and scaling this difference by the standard deviation of the ﬁeld values. That is,\n","\n","\\begin{equation}\n","    X^{*} = \\frac{X-mean(x)}{SD(X)}\n","\\end{equation}\n","\n","For example, consider the acceleration variable from the cars data set, which\n","measures how long (in seconds) each automobile takes to reach 60 miles per hour.\n","Let’s ﬁnd the min–max normalization for three automobiles having times-to-60 of 8,\n","15.548, seconds, and 25 seconds, respectively."]},{"cell_type":"code","metadata":{"id":"2VWHHZHkV4ns"},"source":["standardized_df = df.copy()\n","\n","standardized_df['acceleration']=(df['acceleration']-df['acceleration'].mean())/(df['acceleration'].std())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3pPe9MzhpwJb"},"source":["To summarize, Z-score standardization values will usually range between –4\n","and 4, with the mean value having a Z-score standardization of zero. Figure\n","8 is a histogram of the time-to-60 variable after the Z-score standardization of\n","each field value. Note that the distribution is centered about zero and that the\n","minimum and maximum agree with what we found above."]},{"cell_type":"code","metadata":{"id":"lDW3fp1EWHpE"},"source":["standardized_df['acceleration'].describe()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jr0v317brofz"},"source":["## **NUMERICAL METHODS FOR IDENTIFYING OUTLIERS**\n","\n","One method of using statistics to identify outliers is to use Z-score standard-\n","ization. Often, an outlier can be identified because it is much farther than 3\n","standard deviations from the mean and therefore has a Z-score standardization\n","that is either less than -3 or greater than 3. Field values with Z-scores much\n","beyond this range probably bear further investigation to verify that they do not\n","represent data entry errors or other issues. For example, the vehicle that takes\n","its time (25 seconds) getting to 60 mph had a Z-score of 3.247. This value is\n","greater than 3 (although not by much), and therefore this vehicle is identified by\n","this method as an outlier. The data analyst may wish to investigate the validity\n","of this data value or at least suggest that the vehicle get a tune-up!"]},{"cell_type":"markdown","metadata":{"id":"FYmYA4QdryLa"},"source":["### **Interquartile range**\n","\n","The quartiles of a data set divide the data set into four parts, each containing 25\\% of the data.\n","\n","* The ﬁrst quartile (Q1) is the 25th percentile.\n","* The second quartile (Q2) is the 50th percentile, that is, the median.\n","* item The third quartile (Q3) is the 75th percentile.\n","\n","The interquartile range (IQR) is a measure of variability that is much more robust than the standard deviation. The IQR is calculated as IQR = Q3 - Q1 and may be interpreted to represent the spread of the middle 50\\% of the data."]},{"cell_type":"code","metadata":{"id":"MqVobx0iXUxL"},"source":["df['acceleration'].describe()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8gAQ8wdnsO4v"},"source":["### **Outlier detection**\n","\n","A robust measure of outlier detection is therefore defined as follows. A data\n","value is an outlier if:\n","* It is located 1.5(IQR) or more below Q1, or\n","* It is located 1.5(IQR) or more above Q3."]},{"cell_type":"code","metadata":{"id":"91pDqEE9YN7U"},"source":["import numpy as np\n","import matplotlib.pylab as plt\n","\n","\n","fig = plt.figure(1, figsize=(6, 7))\n","ax = fig.add_subplot(111)\n","\n","ax.set_title('Outliers can be seen both above and below the IQR')\n","\n","ax.boxplot(df['acceleration'], vert=True, manage_ticks=True)\n","ax.set_ylabel('Inter Quantile Range Values')\n","# ax.set_yticklabels(['Min','Q1','Median', 'Q3','Max'])\n","ax.set_xticklabels(['acceleration'])\n","\n","quantiles = np.quantile(df['acceleration'], np.array([0.00, 0.25, 0.50, 0.75, 1.00]))\n","\n","ax.hlines(quantiles, [0] * quantiles.size, [1] * quantiles.size,\n","          color='b', ls=':', lw=0.5, zorder=0)\n","ax.set_xlim(0.5, 1.5)\n","ax.set_yticks(quantiles)\n","plt.show()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jOEqnLsGsfGu"},"source":["# **That's all for today!!**"]},{"cell_type":"markdown","metadata":{"id":"JCvoEaUS7ghr"},"source":["# **Tasks**\n","\n","1. Take into consideration the \"[Cars](https://drive.google.com/file/d/1-FWxPlHngZqZ604_u5AbY7spo8CMYfDf/view?usp=sharing)\" dataset and do the following:\n","    * Create histograms for all numerical valued fields in the cars dataset.\n","    * Can you find some more outliers? Take into consideration other variables in the dataset to check for outliers. Use all possible graphical and numerical analysis.\n","    * Which variables need normalization and standardization in the dataset? perform all needed normalizations and standardizations.\n","\n","2. There is a new type of data transformation technique, called the robust scaling or Robust Scalar. The mathematical formulae of the scalar is as follows:\n","\\begin{equation}\n","X^{*}= \\frac{X-Q_{1}(X)}{Q_3(X)-Q_{1}(X)}\n","\\end{equation}\n","Make an implementation of this transformation scalar and apply it to the cars dataset. Review any observations that you got after the transformation.\n","\n","P.S.: You can already get an implemented version of it in scikit-learn. The details of the function can be found here - [scikit-learn/RobustScalar](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html)"]}]}