{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[{"file_id":"18ncMGtY2tQL80wUapsVSCNHN41Qtrw1Y","timestamp":1701812764873}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"HWRzJ7L7Poq5"},"source":["# **CSI 382 - Data Mining and Knowledge Discovery**"]},{"cell_type":"markdown","metadata":{"id":"szfybLmlPsj1"},"source":["# **Lab 5 - k-Nearest Neighbor Algorithm**\n","\n","k-nearest neighbor algorithm, which is most often used for classification, al-\n","though it can also be used for estimation and prediction. k-Nearest neighbor is\n","an example of instance-based learning, in which the training data set is stored,\n","so that a classification for a new unclassified record may be found simply by\n","comparing it to the most similar records in the training set."]},{"cell_type":"markdown","metadata":{"id":"bCtu167FQEU4"},"source":["# **Dataset for Lab 5**\n","\n","Since as a beginner in data mining it would be a great opportunity to try some techniques to predict the outcome of the drugs that might be accurate for the patient.\n","\n","The target feature is: **Drug type**\n","\n","The feature sets are:\n","* Age\n","* Sex\n","* Blood Pressure Levels (BP)\n","* Cholesterol Levels\n","* Na to Potassium Ration\n","\n","The dataset can be found here in this [URL](https://drive.google.com/file/d/1BWeCLtgyt4B1poaSRwQ-nMLYJfbykYPm/view?usp=sharing)"]},{"cell_type":"markdown","metadata":{"id":"9YNyCq-8Q36n"},"source":["**For today we need the upgraded matplotlib package. So we need to run the following code. This might not be needed if you are running this in a local environment. We need atleaset matplotlib 3.4**"]},{"cell_type":"code","metadata":{"id":"GaPM7TxTN9r_"},"source":["!pip install matplotlib --upgrade"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_lOFRuZoRMhT"},"source":["## **Loading the dataset**"]},{"cell_type":"code","metadata":{"id":"qPCybQ1T0IxF"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZRtXGIHt0QRQ"},"source":["import pandas as pd\n","\n","df = pd.read_csv('/content/drive/MyDrive/drug200.csv')\n","\n","#Check number of rows and columns in the dataset\n","print(\"The dataset has %d rows and %d columns.\" % df.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b0KZqJA7RUWm"},"source":["## **Dataset Preprocessing**\n","\n","We need to transform all categorical data to numerical ones. That's why we are applying some Lambda functions to our dataset columns."]},{"cell_type":"code","metadata":{"id":"5lc8-ZOM4iqz"},"source":["# 0 = Female, 1 = Male\n","df[\"Sex\"] = df[\"Sex\"].apply(lambda x: 0 if x==\"F\" else 1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x0542ReZ4u8i"},"source":["# 0 = LOW, 1 = NORMAL and 2 = HIGH\n","df[\"BP\"] = df[\"BP\"].apply(lambda x: 0 if x==\"LOW\" else (1 if x==\"NORMAL\" else 2))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FSkyfYRZ5f9e"},"source":["# 0 = LOW, 1 = NORMAL and 2 = HIGH\n","df[\"Cholesterol\"] = df[\"Cholesterol\"].apply(lambda x: 0 if x==\"LOW\" else (1 if x==\"NORMAL\" else 2))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rh9K9lGB5lx4"},"source":["# 0 = drugA, 1 = drugB, 2 = drugC, 3 = drugX, 4 = DrugY\n","df[\"Drug\"] = df[\"Drug\"].apply(lambda x: 0 if x==\"drugA\" else (1 if x==\"drugB\" else (2 if x==\"drugC\" else (3 if x==\"drugX\" else 4))))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TIYYXCYq0-yh"},"source":["# checking the columns now\n","\n","df.columns"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Mw2HaVukXqfY"},"source":["df.head(10)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tXTjy00sSeQC"},"source":["# **k-nearest neighbor**\n","\n","k-nearest neighbor algorithm, which is most often used for classification, al-\n","though it can also be used for estimation and prediction. k-Nearest neighbor is\n","an example of instance-based learning, in which the training data set is stored,\n","so that a classification for a new unclassified record may be found simply by\n","comparing it to the most similar records in the training set.\n","Let’s consider an example."]},{"cell_type":"markdown","metadata":{"id":"2_ZTY0ZiSKca"},"source":["For example, in the medical field, suppose that we are interested in classifying\n","the type of drug a patient should be prescribed, based on certain patient charac-\n","teristics, such as the age of the patient and the patient’s sodium/potassium ratio.\n","Figure below is a scatter plot of patients’ sodium/potassium ratio against patients’\n","ages for a sample of 200 patients. The particular drug prescribed is symbolized\n","by the shade of the points."]},{"cell_type":"code","metadata":{"id":"DNbk4ryI0waJ"},"source":["import matplotlib.pyplot as plt\n","\n","fig, ax = plt.subplots()\n","fig.set_size_inches(18.5, 10.5)\n","\n","ax.set_facecolor('xkcd:grey')\n","scatter = plt.scatter( df['Age'],df['Na_to_K'], c=df['Drug'])\n","plt.xlabel('Age')\n","plt.ylabel('Na/K Ratio')\n","plt.grid(True)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QMjkev3Wmc7v"},"source":["def configure_plotly_browser_state():\n","  import IPython\n","  display(IPython.core.display.HTML('''\n","        <script src=\"/static/components/requirejs/require.js\"></script>\n","        <script>\n","          requirejs.config({\n","            paths: {\n","              base: '/static/base',\n","              plotly: 'https://cdn.plot.ly/plotly-1.5.1.min.js?noext',\n","            },\n","          });\n","        </script>\n","        '''))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0qJaBlQ6aViN"},"source":["#plot libaries\n","import numpy as np\n","import plotly\n","import plotly.graph_objs as go\n","import plotly.figure_factory as ff\n","from plotly.offline import init_notebook_mode\n","\n","configure_plotly_browser_state()\n","\n","init_notebook_mode(connected=False)# to show plots in notebook\n","\n","\n","\n","layout = dict(\n","    yaxis=dict(\n","        title='Na/K Ratio',\n","        automargin=True,\n","    ),\n","    xaxis=dict(\n","        title='Age',\n","        automargin=True,\n","    ),\n",")\n","fig = go.Figure(layout=layout)\n","# Add traces\n","\n","fig.add_trace(go.Scatter(x=df['Age'], y=df['Na_to_K'],text= df['Drug'],\n","                    mode='markers',\n","                    name='Drug',\n","                    hovertemplate=\"%{text}\",\n","                    marker=dict(\n","                        size=8,\n","                        color = df['Drug'],\n","                        colorscale='rainbow', # one of plotly colorscales\n","                        showscale=True\n","                    ))\n","            )\n","fig.update_layout(\n","    autosize=False,\n","    width=800,\n","    height=800\n","    )\n","fig.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CXA1cub3SXZD"},"source":["Now suppose that we have a new patient record, without a drug classification,\n","and would like to classify which drug should be prescribed for the patient based\n","on which drug was prescribed for other patients with similar attributes. Identified\n","as “new patient 1,” this patient is 40 years old and has a Na/K ratio of 29, placing\n","her at the center of the circle indicated for new patient 1 in Figure 5.6. Which\n","drug classification should be made for new patient 1? Since her patient profile\n","places her deep into a section of the scatter plot where all patients are prescribed\n","drug Y, we would thereby classify new patient 1 as drug Y. All of the points\n","nearest to this point, that is, all of the patients with a similar profile (with respect\n","to age and Na/K ratio) have been prescribed the same drug, making this an easy\n","classification."]},{"cell_type":"code","metadata":{"id":"XwWVjMsKlkPZ"},"source":["df2 = pd.DataFrame([[40, 1,1,2,29,6],[17, 0,1,0,12.5,6],[47, 1,0,1,13.5,6]], columns=['Age','Sex','BP','Cholesterol','Na_to_K','Drug'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fHKwISt6mrwY"},"source":["new_df = df.copy()\n","new_df=new_df.append(df2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"re5flgFnS0Vh"},"source":["Next, we move to new patient 2, who is 17 years old with a Na/K ratio of 12.5.\n","Suppose we let k = 1 for our k-nearest neighbor algorithm, so that new patient 2\n","would be classified according to whichever single (one) observation it was closest to. In this case, new patient 2 would be classified for drugs B and C, since that is the classification of the point closest to the point on the scatter plot for new patient 2.\n","\n","However, suppose that we now let k = 2 for our k-nearest neighbor algorithm,\n","so that new patient 2 would be classified according to the classification of the\n","k = 2 points closest to it. One of these points is dark gray, and one is medium\n","gray, so that our classifier would be faced with a decision between classifying\n","new patient 2 for drugs B and C or drugs A and X. How would the classifier\n","decide between these two classifications? Voting would not help, since there is\n","one vote for each of two classifications.\n","\n","Voting would help, however, if we let k = 3 for the algorithm, so that new patient 2 would be classified based on the three points closest to it. Since two of the three closest points are same, a classification based on voting would therefore choose drugs A and X as the classification for new patient 2. Note that the classification assigned for new patient 2 differed based on which value we chose for k."]},{"cell_type":"markdown","metadata":{"id":"cnE2nmd9TLBC"},"source":["Finally, consider new patient 3, who is 47 years old and has a Na/K ratio of\n","13.5. Figure 11 presents a close-up of the three nearest neighbors to new patient 3. For k = 1, the k-nearest neighbor algorithm would choose the dark gray (drugs B and C) classification for new patient 3, based on a distance measure. For k = 2, however, voting would not help. But voting would not help for k = 3 in this case either, since the three nearest neighbors to new patient 3 are of three different classifications."]},{"cell_type":"code","metadata":{"id":"d2E7MYg-0FhT"},"source":["#plot libaries\n","import numpy as np\n","import plotly\n","import plotly.graph_objs as go\n","import plotly.figure_factory as ff\n","from plotly.offline import init_notebook_mode\n","\n","configure_plotly_browser_state()\n","\n","init_notebook_mode(connected=False)# to show plots in notebook\n","\n","\n","\n","layout = dict(\n","    yaxis=dict(\n","        title='Na/K Ratio',\n","        automargin=True,\n","    ),\n","    xaxis=dict(\n","        title='Age',\n","        automargin=True,\n","    ),\n",")\n","fig = go.Figure(layout=layout)\n","# Add traces\n","\n","fig.add_trace(go.Scatter(x=new_df['Age'], y=new_df['Na_to_K'],text= new_df['Drug'],\n","                    mode='markers',\n","                    name='Drug',\n","                    hovertemplate=\"%{text}\",\n","                    marker=dict(\n","                        size=8,\n","                        color = new_df['Drug'],\n","                        colorscale='rainbow', # one of plotly colorscales\n","                        showscale=True\n","                    ))\n","            )\n","fig.update_layout(\n","    autosize=False,\n","    width=800,\n","    height=800\n","    )\n","fig.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JnIokBccTSfb"},"source":["This example has shown us some of the issues involved in building a classifier\n","using the k-nearest neighbor algorithm. These issues include:\n","* How many neighbors should we consider? That is, what is k?\n","* How do we measure distance?\n","* How do we combine the information from more than one observation?\n","\n","Later we consider other questions, such as:\n","\n","* Should all points be weighted equally, or should some points have more\n","influence than others?"]},{"cell_type":"markdown","metadata":{"id":"JLX60n_JTgez"},"source":["## **Preparing dataset to be fed into Model**\n","\n","The target/response variable in our dataset is **Drug**. So we are putting the drug labels in our target varible $y$.\n","\n","The other varaibles/predictors are the columns **[Age, Sex, BP, Cholesterol, Na_to_K]** and should be put in our training variable $X$."]},{"cell_type":"code","metadata":{"id":"nyT8sh0i00su"},"source":["y = df['Drug']\n","X = df.drop(columns=['Drug'])\n","\n","print(\"Data shape: \", X.shape)\n","print(\"Labels shape: \", y.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nMynOVcsUh_O"},"source":["## **Supervised Learning**\n","\n","Most data mining methods are supervised methods, however, meaning that (1)\n","there is a particular pre-specified target variable, and (2) the algorithm is given many examples where the value of the target variable is provided, so that the algorithm may learn which values of the target variable are associated with which values of the predictor variables."]},{"cell_type":"markdown","metadata":{"id":"2SQEJga4UsQi"},"source":["### **Training Set**\n","\n","First, the algorithm is provided with a training set of data, which includes the\n","pre-classified values of the target variable in addition to the predictor variables.\n","\n","For example, if we are interested in classifying income bracket, based on age,\n","gender, and occupation, our classification algorithm would need a large pool of\n","records, containing complete (as complete as possible) information about every\n","field, including the target field, income bracket. In other words, the records in the training set need to be pre-classified. A provisional data mining model is then constructed using the training samples provided in the training data set."]},{"cell_type":"markdown","metadata":{"id":"ScoQQA2EU_vs"},"source":["#### **Training Set - Necessarily Incomplete**\n","\n","However, the training set is necessarily incomplete; that is, it does not include the “new” or future data that the data modelers are really interested in classifying. Therefore, the algorithm needs to guard against “memorizing” the training set and blindly applying all patterns found in the training set to the future data.\n","\n","For example, it may happen that all customers named “David” in a training set\n","may be in the high income bracket. We would presumably not want our final\n","model, to be applied to new data, to include the pattern “If the customer’s first name is David, the customer has a high income.” Such a pattern is a spurious artifact of the training set and needs to be verified before deployment."]},{"cell_type":"markdown","metadata":{"id":"LMXs5xE2VCSZ"},"source":["### **Testing Set**\n","\n","Therefore, the next step in supervised data mining methodology is to examine\n","how the provisional data mining model performs on a test set of data. In the test set, a holdout data set, the values of the target variable are hidden temporarily from the provisional model, which then performs classification according to the patterns and structure it learned from the training set. The efficacy of the classifications are then evaluated by comparing them against the true values of the target variable. The provisional data mining model is then adjusted to minimize the error rate on the test set."]},{"cell_type":"code","metadata":{"id":"z8X_aXOn0N8w"},"source":["# Splittng train:test in 90:10 ratio\n","\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=42)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hxO3i-Ot1eqw"},"source":["print(X_train.shape)\n","print(y_train.shape)\n","print(X_test.shape)\n","print(y_test.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3XIAfOk1WPpt"},"source":["## **COMBINATION FUNCTION**"]},{"cell_type":"markdown","metadata":{"id":"_IL-91paWbzC"},"source":["### **Simple Unweighted Voting**\n","1. Before running the algorithm, decide on the value of k, that is, how many\n","records will have a voice in classifying the new record.\n","2. Then, compare the new record to the k nearest neighbors, that is, to the k\n","records that are of minimum distance from the new record in terms of the\n","Euclidean distance or whichever metric the user prefers.\n","3. Once the k records have been chosen, then for simple unweighted voting,\n","their distance from the new record no longer matters. It is simple one\n","record, one vote."]},{"cell_type":"markdown","metadata":{"id":"205qOMeOWdZ5"},"source":["### **Weighted Voting**\n","\n","One may feel that neighbors that are closer or more similar to the new record\n","should be weighted more heavily than more distant neighbors.\n","\n","In weighted voting, the influence of a particular record is inversely proportional to the distance of the record from the new record to be classified."]},{"cell_type":"markdown","metadata":{"id":"6qU2_0MvW7JC"},"source":["**In our model today, we will calculate for both simple unweigted and weighted voting techniques.**"]},{"cell_type":"markdown","metadata":{"id":"RQ5ILe9IWy7w"},"source":["## **CHOOSING k**\n","\n","How should one go about choosing the value of k? In fact, there may not be\n","an obvious best solution. Consider choosing a small value for k. Then it is\n","possible that the classification or estimation may be unduly affected by outliers or unusual observations (“noise”). With small k (e.g., k = 1), the algorithm will simply return the target value of the nearest observation, a process that may lead the algorithm toward overfitting, tending to memorize the training data set at the expense of generalizability."]},{"cell_type":"markdown","metadata":{"id":"c3h6pz0hXHG6"},"source":["**In our model today, we will experiment with k= 2 to 20.**"]},{"cell_type":"markdown","metadata":{"id":"bcE5D_-lVSXU"},"source":["## **Running the model**\n","\n","We will run our model in different scenarios. The scenarios are as follows:\n","\n","**Scenario 1** - We will train our model with **all available data** and calculate it for different $k$'s. We will also calculate the accuracy for each configuration and plot it in Matplotlib with **all available data**.\n","\n","**Scenario 2** - We will train our model with **training dataset** and calculate it for different $k$'s. We will also calculate the accuracy for each configuration and plot it in Matplotlib with our **testing dataset**."]},{"cell_type":"markdown","metadata":{"id":"CkEnc9ClV9La"},"source":["### **Scenario 1 - Training and testing with all available data**"]},{"cell_type":"code","metadata":{"id":"1Lzdo6Qn1pHr"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from matplotlib.colors import ListedColormap\n","from sklearn.neighbors import KNeighborsClassifier\n","\n","score = []\n","\n","for n_neighbors in range(2,21):\n","    for weights in ['uniform', 'distance']:\n","        # we create an instance of Neighbours Classifier and fit the data.\n","        clf = KNeighborsClassifier(n_neighbors, weights=weights)\n","        clf.fit(X, y)\n","        score.append(clf.score(X,y))\n","\n","score = np.reshape(score,(-1,2))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rch8KbsgeVl5"},"source":["score"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0922mc5-EYYM"},"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","\n","\n","labels = np.arange(2, 21, 1)\n","uniform = score[:, 0::2].reshape(-1).tolist()\n","distance = score[:, 1::2].reshape(-1).tolist()\n","x = np.arange(len(labels))  # the label locations\n","width = 0.35  # the width of the bars\n","\n","\n","\n","fig, ax = plt.subplots(figsize=(18,8))\n","rects1 = ax.bar(x - width/2, uniform, width, label='Simple unweighted voting')\n","rects2 = ax.bar(x + width/2, distance, width, label='Weighted Voting')\n","\n","# Add some text for labels, title and custom x-axis tick labels, etc.\n","ax.set_xlabel('N_neighbors')\n","ax.set_ylabel('Scores')\n","ax.set_title('Scores by weights and accuracy')\n","ax.set_xticks(x)\n","ax.set_xticklabels(labels)\n","ax.legend()\n","\n","ax.bar_label(rects1, padding=3)\n","ax.bar_label(rects2, padding=3)\n","\n","fig.tight_layout()\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"The5KoxAXjPa"},"source":["### **Scenario 2 - Training with training data($90\\%$) and testing with testing data($10\\%$)**"]},{"cell_type":"code","metadata":{"id":"mhKAjEWfMwts"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from matplotlib.colors import ListedColormap\n","from sklearn import neighbors\n","\n","score = []\n","\n","for n_neighbors in range(2,21):\n","    for weights in ['uniform', 'distance']:\n","        # we create an instance of Neighbours Classifier and fit the data.\n","        clf = neighbors.KNeighborsClassifier(n_neighbors, weights=weights)\n","        clf.fit(X_train, y_train)\n","        score.append(clf.score(X_test,y_test))\n","\n","score = np.reshape(score,(-1,2))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"StUXuifBfOZn"},"source":["score"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pVnhV5PlM2zj"},"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","\n","\n","labels = np.arange(2, 21, 1)\n","uniform = score[:, 0::2].reshape(-1).tolist()\n","distance = score[:, 1::2].reshape(-1).tolist()\n","x = np.arange(len(labels))  # the label locations\n","width = 0.45  # the width of the bars\n","\n","\n","\n","fig, ax = plt.subplots(figsize=(18,8))\n","rects1 = ax.bar(x - width/2, uniform, width, label='Simple unweighted voting')\n","rects2 = ax.bar(x + width/2, distance, width, label='Weighted Voting')\n","\n","# Add some text for labels, title and custom x-axis tick labels, etc.\n","ax.set_xlabel('N_neighbors')\n","ax.set_ylabel('Scores')\n","ax.set_title('Scores by weights and accuracy')\n","ax.set_xticks(x)\n","ax.set_xticklabels(labels)\n","ax.legend()\n","\n","\n","ax.bar_label(rects1, padding=3)\n","ax.bar_label(rects2, padding=3)\n","\n","fig.tight_layout()\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H4sN4n6oYCM4"},"source":["### **Checking for individual predictions that our model has made**\n","\n","Let's check which drug our model recommends to the 3 new patients in our dataset."]},{"cell_type":"code","source":["clf = neighbors.KNeighborsClassifier(n_neighbors=5, weights='distance')\n","clf.fit(X_train, y_train)"],"metadata":{"id":"k2MqJbtLoTpl"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L-YcSUJQPT99"},"source":["predcitor = df2.drop(columns=[\"Drug\"])\n","\n","pred_labels = clf.predict(predcitor)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l2rhg0XxPh0X"},"source":["pred_labels"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-mR8wngtrnHL"},"source":["## **Visualizing the Decision boundaries (Not in syllabus)**\n","\n","Let's take the first two features in our dataset and plot a decision boundary for 'Age' and 'Sex'."]},{"cell_type":"code","metadata":{"id":"RxFH434mj8oE"},"source":["import plotly.graph_objects as go\n","import numpy as np\n","from sklearn.datasets import make_moons\n","from sklearn.model_selection import train_test_split\n","from sklearn.neighbors import KNeighborsClassifier\n","\n","configure_plotly_browser_state()\n","\n","init_notebook_mode(connected=False)# to show plots in notebook\n","\n","\n","mesh_size = .02\n","margin = 0.25\n","\n","\n","\n","# Load and split data\n","X_m, y_m = X.iloc[:,[0,1]],y\n","\n","# Create a mesh grid on which we will run our model\n","x_min, x_max = X_m.iloc[:, 0].min() - margin, X_m.iloc[:, 0].max() + margin\n","y_min, y_max = X_m.iloc[:, 1].min() - margin, X_m.iloc[:, 1].max() + margin\n","xrange = np.arange(x_min, x_max, mesh_size)\n","yrange = np.arange(y_min, y_max, mesh_size)\n","xx, yy = np.meshgrid(xrange, yrange)\n","\n","# Create classifier, run predictions on grid\n","clf = KNeighborsClassifier(15, weights='distance')\n","clf.fit(X_m, y_m)\n","Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n","Z = Z.reshape(xx.shape)\n","\n","layout = dict(\n","    yaxis=dict(\n","        title='BP',\n","        automargin=True,\n","    ),\n","    xaxis=dict(\n","        title='Age',\n","        automargin=True,\n","    ),\n",")\n","\n","# Plot the figure\n","fig = go.Figure(data=[\n","    go.Contour(\n","        x=xrange,\n","        y=yrange,\n","        z=Z,\n","        colorscale='RdBu'\n","    )\n","],\n","    layout = layout)\n","fig.show()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8wlboB2iYMwt"},"source":["# **That's all for today!**"]}]}