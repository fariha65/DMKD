{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[{"file_id":"1vGYD2ypcscD7sDCeZzR2wIbJ1QhzAeVM","timestamp":1696238908675},{"file_id":"1fLv1gcATGVkqVJyJ1LxxLk5jWQXQHYt-","timestamp":1636476360510},{"file_id":"18ncMGtY2tQL80wUapsVSCNHN41Qtrw1Y","timestamp":1635535081110}],"collapsed_sections":["H5JSY8HVvK9U","E2L1j6tY5LfG"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"HWRzJ7L7Poq5"},"source":["# **CSI 382 - Data Mining and Knowledge Discovery**"]},{"cell_type":"markdown","metadata":{"id":"szfybLmlPsj1"},"source":["# **Lab 7 - Neural Networks**\n","\n","The inspiration for neural networks was the recognition that complex learning systems\n","in animal brains consisted of closely interconnected sets of neurons. Although a particular neuron may be relatively simple in structure, dense networks of interconnected neurons could perform complex learning tasks such as classiﬁcation and pattern recognition. The human brain, for example, contains approximately $10^{11}$ neurons, each connected on average to $10,000$ other neurons, making a total of $1,000,000,000,000,000=10^{15}$ synaptic connections.\n","\n","**Definition**\n","\n","Artiﬁcial neural networks (hereafter, neural networks) represent an attempt at a very basic level to imitate the type of nonlinear learning that occurs in the networks of neurons found in nature."]},{"cell_type":"markdown","metadata":{"id":"bCtu167FQEU4"},"source":["# **Dataset for Lab 7**\n","\n","**Data Set Information:**\n","\n","The examined group comprised kernels belonging to three different varieties of wheat: Kama, Rosa and Canadian, 70 elements each, randomly selected for\n","the experiment. High quality visualization of the internal kernel structure was detected using a soft X-ray technique. It is non-destructive and considerably cheaper than other more sophisticated imaging techniques like scanning microscopy or laser technology. The images were recorded on 13x18 cm X-ray KODAK plates. Studies were conducted using combine harvested wheat grain originating from experimental fields, explored at the Institute of Agrophysics of the Polish Academy of Sciences in Lublin.\n","\n","The data set can be used for the tasks of classification and cluster analysis.\n","\n","**Attribute Information:**\n","\n","To construct the data, seven geometric parameters of wheat kernels were measured:\n","1. area A,\n","2. perimeter P,\n","3. compactness C = 4*pi*A/P^2,\n","4. length of kernel,\n","5. width of kernel,\n","6. asymmetry coefficient\n","7. length of kernel groove.\n","All of these parameters were real-valued continuous.\n","\n","**Relevant Papers:**\n","\n","M. Charytanowicz, J. Niewczas, P. Kulczycki, P.A. Kowalski, S. Lukasik, S. Zak, 'A Complete Gradient Clustering Algorithm for Features Analysis of X-ray Images', in: Information Technologies in Biomedicine, Ewa Pietka, Jacek Kawa (eds.), Springer-Verlag, Berlin-Heidelberg, 2010, pp. 15-24.\n","\n","The dataset can be found here in this [URL](https://drive.google.com/file/d/1ErhaHBOWTTf7648GnAXDI4EhqnP-cimi/view?usp=sharing)"]},{"cell_type":"code","metadata":{"id":"t4yZD0OacRNd"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5Fh8U31FLDD0"},"source":["## **Loading the dataset**"]},{"cell_type":"code","metadata":{"id":"cpBGKLVWQBwV"},"source":["from random import seed\n","from random import randrange\n","from random import random\n","from csv import reader\n","from math import exp"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l1RdQQThaiuL"},"source":["# Load a CSV file\n","def load_csv(filename):\n","\tdataset = list()\n","\twith open(filename, 'r') as file:\n","\t\tcsv_reader = reader(file)\n","\t\tfor row in csv_reader:\n","\t\t\tif not row:\n","\t\t\t\tcontinue\n","\t\t\tdataset.append(row)\n","\treturn dataset"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ai1G5tRcnIYg"},"source":["# Test Backprop on Seeds dataset\n","seed(1)\n","#load and prepare data\n","filename = '/content/drive/MyDrive/wheat-seeds.csv'\n","dataset = load_csv(filename)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"upaLPtTHnLfo"},"source":["dataset[:10]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1xRqKgd-LQGF"},"source":["## **Dataset Preprocessing**\n","\n","As a neural network does not understand anything rather than numbers, then let's convert every entity into numbers first."]},{"cell_type":"code","metadata":{"id":"whQvRz_kUGUH"},"source":["# Convert string column to float\n","def str_column_to_float(dataset, column):\n","\tfor row in dataset:\n","\t\trow[column] = float(row[column].strip())#dataset e unwanted value thakle strip kore strip() diye\n","\n","# Convert string column to integer\n","def str_column_to_int(dataset, column):\n","\tclass_values = [row[column] for row in dataset]\n","\tunique = set(class_values)\n","\tprint(unique)\n","\tlookup = dict()\n","\tfor i, value in enumerate(unique):\n","\t\tlookup[value] = i\n","\tfor row in dataset:\n","\t\trow[column] = lookup[row[column]]\n","\treturn lookup\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9LhIIr8qm_RI"},"source":["for i in range(len(dataset[0])-1):\n","\tstr_column_to_float(dataset, i)\n","\n","# convert class column to integers\n","str_column_to_int(dataset, len(dataset[0])-1)# last column jodi(dataset,7) ota specific detaset er jono"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RZpBYFDIDMix"},"source":["# **Input-Output Encoding**\n","\n","One possible drawback of neural networks is that all attribute values must be encoded in a standardized manner, taking values between zero and 1, even for categorical variables. Later, when we examine the details of the back-propagation algorithm, we shall understand why this is necessary.\n","\n","For now, however, how does one go about standardizing all the attribute values?"]},{"cell_type":"markdown","metadata":{"id":"2Y1XD-oADVvm"},"source":["## **Continuous Variables**\n","\n","For continuous variables, this is not a problem, as we discussed in Lecture 2.\n","We may simply apply the \\textit{min–max normalization}:\n","\\begin{equation*}\n","    X* = \\frac{X-min(X)}{range(X)} = \\frac{X-min(X)}{max(X) - min(X)}\n","\\end{equation*}\n","This works well as long as the minimum and maximum values are known and all\n","potential new data are bounded between them. Neural networks are somewhat robust to minor violations of these boundaries. If more serious violations are expected,\n","certain ad hoc solutions may be adopted, such as rejecting values that are outside the\n","boundaries, or assigning such values to either the minimum or maximum value."]},{"cell_type":"markdown","metadata":{"id":"IjL6tMZpD2x-"},"source":["## **Categorical Variables**\n","\n","Categorical variables are more problematical, as might be expected. If the number of possible categories is not too large, one may use indicator (ﬂag) variables.\n","\n","\n","For example, many data sets contain a $gender$ attribute, containing values $female$, $male$,\n","and $unknown$. Since the neural network could not handle these attribute values in their\n","present form, we could, instead, create $indicator$ variables for female and male. Each\n","record would contain values for each of these two $indicator$ variables. Records for\n","females would have a value of $1$ for $female$ and 0 for $male$, while records for males\n","would have a value of $0$ for $female$ and $1$ for $male$. Records for persons of unknown\n","gender would have values of 0 for $female$ and 0 for $male$.\n","\n","\n","In general, categorical\n","variables with $k$ classes may be translated into $k - 1$ indicator variables, as long as\n","the deﬁnition of the indicators is clearly deﬁned."]},{"cell_type":"code","metadata":{"id":"ahU3EJFFUTz4"},"source":["# Find the min and max values for each column\n","def dataset_minmax(dataset):\n","\tminmax = list()\n","\tstats = [[min(column), max(column)] for column in zip(*dataset)]\n","\treturn stats\n","\n","# Rescale dataset columns to the range 0-1\n","def normalize_dataset(dataset, minmax):\n","\tfor row in dataset:\n","\t\tfor i in range(len(row)-1):\n","\t\t\trow[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dboFAHsNyumZ"},"source":["# normalize input variables\n","minmax = dataset_minmax(dataset)\n","normalize_dataset(dataset, minmax)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jAellggrLlct"},"source":["dataset[:10]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8APv8RVvEM_f"},"source":["## **Output**\n","\n","With respect to output, we shall see that neural network output nodes always\n","return a continuous value between zero and 1 as output.\n","\n","**How can we use such continuous output for classiﬁcation?**\n","\n","Many classiﬁcation problems have a dichotomous result, an up-or-down decision, with only two possible outcomes. For example, “Is this customer about to leave our company’s service?” For dichotomous classiﬁcation problems, one option is to use a single output node , with a threshold value set a priori which would separate the classes, such as “leave” or “stay.” For example, with the threshold of “leave if output $\\geq$ 0.67,” an output of 0.72 from the output node would classify that record as likely to leave the company’s service."]},{"cell_type":"markdown","metadata":{"id":"hPn3gbuKL1Uy"},"source":["# **Neural Network**"]},{"cell_type":"markdown","metadata":{"id":"wyZJamV2L40X"},"source":["## **Number of Nodes**\n","\n","The number of input nodes usually depends on the number and type of attributes\n","in the data set.\n","\n","The number of hidden layers, and the number of nodes in each hidden\n","layer, are both conﬁgurable by the user.\n","\n","One may have more than one node in the\n","output layer, depending on the particular classiﬁcation task at hand."]},{"cell_type":"markdown","metadata":{"id":"RkqxlRcpMA3e"},"source":["### **Hidden Layer**\n","\n","Since more nodes in the\n","hidden layer increases the power and ﬂexibility of the network for identifying complex\n","patterns, one might be tempted to have a large number of nodes in the hidden layer.\n","On the other hand, an overly large hidden layer leads to over-fitting, memorizing\n","the training set at the expense of generalizability to the validation set. If over-fitting\n","is occurring, one may consider reducing the number of nodes in the hidden layer;\n","conversely, if the training accuracy is unacceptably low, one may consider increasing\n","the number of nodes in the hidden layer."]},{"cell_type":"markdown","metadata":{"id":"-xwC9ptmMGlE"},"source":["### **Input Layer**\n","\n","The input layer accepts inputs from the data set, such as attribute values, and\n","simply passes these values along to the hidden layer without further processing. Thus,\n","the nodes in the input layer do not share the detailed node structure that the hidden\n","layer nodes and the output layer nodes share."]},{"cell_type":"markdown","metadata":{"id":"DYp5roH6U_Ew"},"source":["# **Neural Networks - Working Procedure**\n","\n"]},{"cell_type":"markdown","metadata":{"id":"y2MgkoRsVGCD"},"source":["First, a combination function (usually summation, $\\sum$) produces a linear combination of the node inputs and the connection weights into a single scalar value, which we will term $net$. Thus, for a given node $j$,\n","\n","$ net_j = \\sum_{i}{W_{ij}x_{ij} = W_{0j}x_{0j} + W_{1j}x_{1j}+ \\dots +W_{Ij}x_{Ij}} $\n","\n","where $x_{ij}$ represents the $i^{th}$ input to node $j$, $W_{ij}$ represents the weight associated with the $i^{th}$ input to node $j$, and there are $I + 1$ inputs to node $j$. Note that $x_1 , x_2 , \\dots , x_I$ represent inputs from upstream nodes, while $x_0$ represents a constant input, analogous to the constant factor in regression models, which by convention uniquely takes the value $x_{0j} = 1$. Thus, each hidden layer or output layer node $j$ contains an “extra” input equal to a particular weight $W_{0j}x_{0j} = W_{0j} $, such as $W_{0B}$ for node $B$."]},{"cell_type":"code","metadata":{"id":"SDPdy5QWXHLu"},"source":["# Initialize a network\n","def initialize_network(n_inputs, n_hidden, n_outputs):\n","\tnetwork = list()\n","\thidden_layer = [{'weights':[random() for i in range(n_inputs + 1)]} for i in range(n_hidden)]\n","\tnetwork.append(hidden_layer)\n","\toutput_layer = [{'weights':[random() for i in range(n_hidden + 1)]} for i in range(n_outputs)]\n","\tnetwork.append(output_layer)\n","\treturn network"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SbZVAb6IW-1n"},"source":["# Update network weights with error\n","def update_weights(network, row, l_rate):\n","\tfor i in range(len(network)):\n","\t\tinputs = row[:-1]\n","\t\tif i != 0:\n","\t\t\tinputs = [neuron['output'] for neuron in network[i - 1]]\n","\t\tfor neuron in network[i]:\n","\t\t\tfor j in range(len(inputs)):\n","\t\t\t\tneuron['weights'][j] -= l_rate * neuron['delta'] * inputs[j]\n","\t\t\tneuron['weights'][-1] -= l_rate * neuron['delta']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I07T7GbvVXUu"},"source":["For example, for node $A$ in the hidden layer, we have\n","\\begin{equation*}\n","    \\begin{split}\n","        net_A &= \\sum_{i}{W_{iA}x_{iA} = W_{0A}(1) + W_{1A}x_{1A}+ W_{2A}x_{2A}+ W_{3A}x_{3A}}\\\\\n","        &= 0.5 + 0.6(0.4) + 0.8(0.2) + 0.6(0.7) = 1.32\n","    \\end{split}\n","\\end{equation*}\n","Within node $A$, this combination function $net_A = 1.32$ is then used as an input to an activation function.\n","\n","In biological neurons, signals are sent between neurons when the\n","combination of inputs to a particular neuron cross a certain threshold, and the neuron\n","“ﬁres.”\n","\n","This is nonlinear behavior, since the ﬁring response is not necessarily linearly\n","related to the increment in input stimulation. Artiﬁcial neural networks model this\n","behavior through a **nonlinear activation function**."]},{"cell_type":"markdown","metadata":{"id":"x-ctqYF9Wk2b"},"source":["# **Sigmoid Activation Function**"]},{"cell_type":"markdown","metadata":{"id":"3bHV3V5jVtV-"},"source":["The most common activation function is the sigmoid function:\n","\\begin{equation*}\n","    \\begin{split}\n","        y &= \\frac{1}{1+e^{-x}}\n","    \\end{split}\n","\\end{equation*}\n","where $e$ is base of natural logarithms, equal to about 2.718281828. Thus, within\n","node $A$, the activation would take $net_A = 1.32$ as input to the sigmoid activation\n","function, and produce an output value of $y = \\frac{1}{(1 + e^{-1.32} )} = 0.7892$.\n","\n","Node $A$’s\n","work is done (for the moment), and this output value would then be passed along the\n","connection to the output node $Z$, where it would form (via another linear combination)\n","a component of $net_Z$."]},{"cell_type":"code","metadata":{"id":"HTRtI_bhViRc"},"source":["# Calculate neuron activation for an input\n","def activate(weights, inputs):\n","\tactivation = weights[-1]\n","\tfor i in range(len(weights)-1):\n","\t\tactivation += weights[i] * inputs[i]\n","\treturn activation\n","\n","# Transfer neuron activation\n","def transfer(activation):\n","\treturn 1.0 / (1.0 + exp(-activation))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"71KQZrOBV5Ry"},"source":["But before we can compute $net_Z$, we need to ﬁnd the contribution of node $B$.\n","From the values in Figure, we have,\n","\n","\\begin{equation*}\n","    \\begin{split}\n","        net_B &= \\sum_{i}{W_{iB}x_{iB} = W_{0B}(1) + W_{1B}x_{1B}+ W_{2B}x_{2B}+ W_{3B}x_{3B}}\\\\\n","        &= 0.7 + 0.9(0.4) + 0.8(0.2) + 0.4(0.7) = 1.5\n","    \\end{split}\n","\\end{equation*}\n","Then,\n","\\begin{equation*}\n","    \\begin{split}\n","        f(net_B) &= \\frac{1}{1+e^{-1.5}} = 0.8176\n","    \\end{split}\n","\\end{equation*}\n","\n","Node $Z$ then combines these outputs from nodes $A$ and $B$, through $net_Z$, a weighted\n","sum, using the weights associated with the connections between these nodes."]},{"cell_type":"markdown","metadata":{"id":"_504kmz-WJnS"},"source":["Note\n","that the inputs $x_i$ to node $Z$ are not data attribute values but the outputs from the sigmoid functions from upstream nodes:\n","\\begin{equation*}\n","    \\begin{split}\n","        net_Z &= \\sum_{i}{W_{iZ}x_{iZ} = W_{0Z}(1) + W_{AZ}x_{AZ}+ W_{BZ}x_{BZ}}\\\\\n","        &= 0.5 + 0.9(0.7892) + 0.9(0.8176) = 1.9461\n","    \\end{split}\n","\\end{equation*}\n","Finally, $net_Z$ is input into the sigmoid activation function in node $Z$, resulting in\n","\\begin{equation*}\n","    \\begin{split}\n","        f(net_Z) &= \\frac{1}{1+e^{-1.9461}} = 0.8750\n","    \\end{split}\n","\\end{equation*}\n","\n","This value of 0.8750 is the output from the neural network for this ﬁrst pass through\n","the network, and represents the value predicted for the target variable for the ﬁrst\n","observation."]},{"cell_type":"markdown","metadata":{"id":"ByKQdzJlWU9R"},"source":["**Why use the sigmoid function?**\n","\n","Because it combines nearly linear behavior, curvilinear\n","behavior, and nearly constant behavior, depending on the value of the input.\n","\n","Figure \\ref{fig:sigmoid1} shows the graph of the sigmoid function $ y = f (x) = \\frac{1}{(1 + e^{-x} )}$, for $-5 < x < 5$\n","[although $f(x)$ may theoretically take any real-valued input].\n","\n","Through much of the\n","center of the domain of the input $x$ (e.g., $-1 < x < 1$), the behavior of $f(x)$ is nearly\n","linear. As the input moves away from the center, $f(x)$ becomes curvilinear. By the\n","time the input reaches extreme values, $f(x)$ becomes nearly constant.\n"]},{"cell_type":"markdown","metadata":{"id":"EXFwuAaOXSPp"},"source":["# **Back propagation**\n","\n","**How does a neural network learn?**\n","\n","\n","Neural networks represent a supervised learning\n","method, requiring a large training set of complete records, including the target\n","variable. As each observation from the training set is processed through the network,\n","an output value is produced from the output node.\n","\n","This output value is then compared to the actual value\n","of the target variable for this training set observation, and the error (actual - output)\n","is calculated. This prediction error is analogous to the residuals in regression models."]},{"cell_type":"markdown","metadata":{"id":"SDqQThe7Xjeh"},"source":["To measure how well the output predictions fit the actual target values, most neural\n","network models use the sum of squared errors:\n","\n","$ {SSE} = \\sum_{{records}}{\\sum_{{output nodes}}{{(actual - output)}^2}} $\n","\n","where the squared prediction errors are summed over all the output nodes and over\n","all the records in the training set.\n","\n","The problem is therefore to construct a set of model weights that will minimize\n","the SSE. In this way, the weights are analogous to the parameters of a regression\n","model. The “true” values for the weights that will minimize SSE are unknown, and\n","our task is to estimate them, given the data. However, due to the nonlinear nature of\n","the sigmoid functions permeating the network, there exists no closed-form solution\n","for minimizing SSE as exists for least-squares regression."]},{"cell_type":"code","metadata":{"id":"MTCxunbTYQ6L"},"source":["# Train a network for a fixed number of epochs\n","def train_network(network, train, l_rate, n_epoch, n_outputs):\n","    for epoch in range(n_epoch):\n","        sum_error = 0\n","        for row in train:\n","            outputs = forward_propagate(network, row)\n","            expected = [0 for i in range(n_outputs)]\n","            expected[row[-1]] = 1\n","            sum_error += sum([(expected[i]-outputs[i])**2 for i in range(len(expected))])\n","            backward_propagate_error(network, expected)\n","            update_weights(network, row, l_rate)\n","        print('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5JwBN7GpXyl9"},"source":["# **Gradient Descent Method**\n","\n","We must therefore turn to optimization methods, specifically gradient-descent methods,\n","to help us find the set of weights that will minimize SSE.\n","\n","\n","Suppose that we have a\n","set (vector) of $m$ weights $w = w_0,w_1,w_2, \\dots , w_m$ in our neural network model and\n","we wish to find the values for each of these weights that, together, minimize SSE.\n","We can use the gradient descent method, which gives us the direction that we should\n","adjust the weights in order to decrease SSE. The gradient of SSE with respect to the\n","vector of weights $w$ is the vector derivative:\n","\n","$\n","\\nabla{SSE(W)} =  \\left[  \\frac{ \\partial{SSE}} {\\partial w_0}, \\frac{ \\partial{SSE}} {\\partial w_1},\\dots, \\frac{ \\partial{SSE}} {\\partial w_m}  \\right]\n","$\n","\n","that is, the vector of partial derivatives of SSE with respect to each of the weights."]},{"cell_type":"code","metadata":{"id":"VTaWaVXIbKy1"},"source":["# Forward propagate input to a network output\n","def forward_propagate(network, row):\n","\tinputs = row\n","\tfor layer in network:\n","\t\tnew_inputs = []\n","\t\tfor neuron in layer:\n","\t\t\tactivation = activate(neuron['weights'], inputs)\n","\t\t\tneuron['output'] = transfer(activation)\n","\t\t\tnew_inputs.append(neuron['output'])\n","\t\tinputs = new_inputs\n","\treturn inputs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FGz4qMMFbebQ"},"source":["# Train a network for a fixed number of epochs\n","def train_network(network, train, l_rate, n_epoch, n_outputs):\n","    for epoch in range(n_epoch):\n","        sum_error = 0\n","        for row in train:\n","            outputs = forward_propagate(network, row)\n","            expected = [0 for i in range(n_outputs)]\n","            expected[row[-1]] = 1\n","            sum_error += sum([(expected[i]-outputs[i])**2 for i in range(len(expected))])\n","            backward_propagate_error(network, expected)\n","            update_weights(network, row, l_rate)\n","        print('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TW4p7l-CYbI3"},"source":["# **Back Propagation Rules**\n","\n","The back-propagation algorithm takes the prediction error (actual - output) for a\n","particular record and percolates the error back through the network, assigning partitioned responsibility for the error to the various connections. The weights on these\n","connections are then adjusted to decrease the error, using gradient descent.\n","\n","Using the sigmoid activation function and gradient descent, Mitchell derives\n","the back-propagation rules as follows:\n","\n","$ w_{ij,new} = w_{ij,current} + \\Delta w_{ij} \\quad \\text{ where,}\n","\\quad \\Delta w_{ij} = \\eta \\delta_{j} x_{ij} $"]},{"cell_type":"markdown","metadata":{"id":"T4KZ1AgeYr4b"},"source":["Now we know that $\\eta$ represents the learning rate and $x_{ij}$ signiﬁes the $i^{{th}}$ input to node\n","$j$, but what does $\\delta_j$ represent? The component $\\delta_j$ represents the responsibility for a\n","particular error belonging to node $j$. The error responsibility is computed using the\n","partial derivative of the sigmoid function with respect to $net_j$ and takes the following\n","forms, depending on whether the node in question lies in the output layer or the hidden\n","layer:\n","\n","\\begin{equation*}\n","    \\delta_j =\n","\\begin{cases}\n","    {output}_j (1 - {output}_j )({actual}_j - {output}_j ) & \\text{for output layer nodes}\\\\\n","    {output}_j (1 -{output}_j) \\sum\\limits_{{downstream}}{W_{jk}\\delta_j} & \\text{for hidden layer nodes}          \n","\\end{cases}\n","\\end{equation*}\n","\n","where $\\sum_{{downstream}}{W_{jk}\\delta_j}$ refers to the weighted sum of the error responsibilities for\n","the nodes downstream from the particular hidden layer node. (For the full derivation,\n","see Mitchell \\cite{mitchell1997machine}.)"]},{"cell_type":"code","metadata":{"id":"lABuNDLVZAnr"},"source":["# Backpropagate error and store in neurons\n","def backward_propagate_error(network, expected):\n","\tfor i in reversed(range(len(network))):\n","\t\tlayer = network[i]\n","\t\terrors = list()\n","\t\tif i != len(network)-1:\n","\t\t\tfor j in range(len(layer)):\n","\t\t\t\terror = 0.0\n","\t\t\t\tfor neuron in network[i + 1]:\n","\t\t\t\t\terror += (neuron['weights'][j] * neuron['delta'])\n","\t\t\t\terrors.append(error)\n","\t\telse:\n","\t\t\tfor j in range(len(layer)):\n","\t\t\t\tneuron = layer[j]\n","\t\t\t\terrors.append(neuron['output'] - expected[j])\n","\t\tfor j in range(len(layer)):\n","\t\t\tneuron = layer[j]\n","\t\t\tneuron['delta'] = errors[j] * transfer_derivative(neuron['output'])\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"I3wEvKg3Zobx"},"source":["# Backpropagation Algorithm With Stochastic Gradient Descent\n","def back_propagation(train, test, l_rate, n_epoch, n_hidden):\n","\tn_inputs = len(train[0]) - 1\n","\tn_outputs = len(set([row[-1] for row in train]))\n","\tnetwork = initialize_network(n_inputs, n_hidden, n_outputs)\n","\ttrain_network(network, train, l_rate, n_epoch, n_outputs)\n","\tpredictions = list()\n","\tfor row in test:\n","\t\tprediction = predict(network, row)\n","\t\tpredictions.append(prediction)\n","\treturn(predictions)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J-hfkYpJZrTw"},"source":["# Calculate the derivative of an neuron output\n","def transfer_derivative(output):\n","\treturn output * (1.0 - output)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9HtvIRuwZMPn"},"source":["# **Termination Criteria**\n","\n","The neural network algorithm would then proceed to work through the training data\n","set, record by record, adjusting the weights constantly to reduce the prediction error.\n","\n","It may take many passes through the data set before the algorithm’s termination\n","criterion is met. What, then, serves as the termination criterion, or stopping criterion?\n","If training time is an issue, one may simply set the number of passes through the\n","data, or the amount of real-time the algorithm may consume, as termination criteria.\n","\n","However, what one gains in short training time is probably bought with degradation\n","in model efficacy."]},{"cell_type":"code","metadata":{"id":"-AstAsKMa1-7"},"source":["# Split a dataset into k folds\n","def cross_validation_split(dataset, n_folds):\n","\tdataset_split = list()\n","\tdataset_copy = list(dataset)\n","\tfold_size = int(len(dataset) / n_folds)\n","\tfor i in range(n_folds):\n","\t\tfold = list()\n","\t\twhile len(fold) < fold_size:\n","\t\t\tindex = randrange(len(dataset_copy))\n","\t\t\tfold.append(dataset_copy.pop(index))\n","\t\tdataset_split.append(fold)\n","\treturn dataset_split\n","\n","# Calculate accuracy percentage\n","def accuracy_metric(actual, predicted):\n","\tcorrect = 0\n","\tfor i in range(len(actual)):\n","\t\tif actual[i] == predicted[i]:\n","\t\t\tcorrect += 1\n","\treturn correct / float(len(actual)) * 100.0\n","\n","# Evaluate an algorithm using a cross validation split\n","def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n","\tfolds = cross_validation_split(dataset, n_folds)\n","\tscores = list()\n","\tfor fold in folds:\n","\t\ttrain_set = list(folds)\n","\t\ttrain_set.remove(fold)\n","\t\ttrain_set = sum(train_set, [])\n","\t\ttest_set = list()\n","\t\tfor row in fold:\n","\t\t\trow_copy = list(row)\n","\t\t\ttest_set.append(row_copy)\n","\t\t\trow_copy[-1] = None\n","\t\tpredicted = algorithm(train_set, test_set, *args)\n","\t\tactual = [row[-1] for row in fold]\n","\t\taccuracy = accuracy_metric(actual, predicted)\n","\t\tscores.append(accuracy)\n","\treturn scores"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JaOeF7xAbvYi"},"source":["# Make a prediction with a network\n","def predict(network, row):\n","\toutputs = forward_propagate(network, row)\n","\treturn outputs.index(max(outputs))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZheMcSbVjAqt"},"source":["# **Learning Rate**\n","\n","Recall that the learning rate $\\eta$, $0 < \\eta < 1$, is a constant chosen to help us move the\n","network weights toward a global minimum for SSE.\n","\n","**However, what value should $\\eta$ take? How large should the weight adjustments be?**\n","\n","When the learning rate is very small, the weight adjustments tend to be very\n","small. Thus, if $\\eta$ is small when the algorithm is initialized, the network will probably\n","take an unacceptably long time to converge. Is the solution therefore to use large\n","values for $\\eta$? Not necessarily. Suppose that the algorithm is close to the optimal\n","solution and we have a large value for $\\eta$. This large $\\eta$ will tend to make the algorithm\n","overshoot the optimal solution.\n","\n","Consider Figure 7.5, where $W^*$ is the optimum value for weight $W$, which\n","has current value $W_{current}$. According to the gradient descent rule, $\\Delta w_{current} = -\\eta(\\partial SSE/\\partial w_{current})$, $W_{current}$ will be adjusted in the direction of $W^*$. But if the learning\n","rate $\\eta$, which acts as a multiplier in the formula for  $\\Delta w_{current}$, is too large, the new\n","weight value $W_{new}$ will jump right past the optimal value $W^*$, and may in fact end up\n","farther away from $W^*$ than $W_{current}$."]},{"cell_type":"code","metadata":{"id":"4XW6tNmAZztQ"},"source":["n_folds = 5\n","l_rate = 0.1\n","n_epoch = 600\n","n_hidden = 21\n","scores = evaluate_algorithm(dataset, back_propagation, n_folds, l_rate, n_epoch, n_hidden)\n","print('Scores: %s' % scores)\n","print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8wlboB2iYMwt"},"source":["# **That's all for today!**"]},{"cell_type":"markdown","metadata":{"id":"3ktiGpE-0SKO"},"source":["# **Tasks**\n","\n","Go to this [url](https://drive.google.com/file/d/1PU1AQVCgnFL2XbgIJRl3TwkHY5Ag3qIv/view?usp=sharing) and download the data first. In order to know more about the dataset please refer to these links - [UCI/iris](https://archive.ics.uci.edu/ml/datasets/Iris), or [Kaggle/iris_dataset](https://www.kaggle.com/uciml/iris).\n","\n","**The \"species\" field refers to the Predicted attribute: class of iris plant.**\n","\n"," Now try to do the following:\n","\n","1. Apply an ANN to find/predict/classify the class of IRIS plants.\n","2. Apply different termination criteria to see if the accuracy changes."]}]}